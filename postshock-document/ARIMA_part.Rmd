---
title: "Postshock"
author: "Qiyang Wang"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 4
urlcolor: blue
---
## Nike data
Import the data form NIKE
```{r}
nike<-read.csv("NKE_quarterly_valuation_measures.csv",header = TRUE)
nike1<-nike[,-c(1,2)]
A<-matrix(NA,nrow=ncol(nike1),ncol=3)
for (i in 1:ncol(nike1)) {
  A[i,1]<-colnames(nike1)[i]
  A[i,2]<-nike1[2,i]
  A[i,3]<-nike1[8,i]
  
}
A<-as.data.frame(A)
colnames(A)<-c("date","ev","ev.rev")
A$ev.rev<-as.numeric(A$ev.rev)
A$ev<-as.numeric(gsub(",","",A$ev))
A$date<-gsub("X","",A$date)
A$revenue<-A$ev/A$ev.rev
b<-c()
i=1
for (i in 1:(nrow(A)-1)) {
  b[i]<-((A[i,4]-A[i+1,4])*100/A[i+1,4])/100
}
b[nrow(A)]<-0
A$per<-b
A$per<-round(A$per,3)
Nike.f<-A
Nike.f<-Nike.f[1:which(Nike.f$date=="02.28.2001", arr.ind=TRUE),]
```

## import the dbw and QL loss function
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START QL_loss_function
QL_loss_function <- function(pred, gt){gt/pred - log(gt/pred) - 1}
### END QL_loss_function

#We specify some functions for transforming y

#mean_square_y will be used in garch models
mean_square_y <- function(y){return((y-mean(y))**2)}

#identity function will be used in synthetic prediction models
id <- function(y){return(y)}

```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START dbw
dbw <- function(X                         # List of covariates for time series; X[[1]] is target, X[[2:n+1]] are donors
                ,dbw_indices              # Indices of covariates to use
                ,shock_time_vec           # Vector of shock times for each series
                ,scale = FALSE            # Whether to scale data for standardization
                ,center = FALSE           # Whether to center data (subtract mean)
                ,sum_to_1 = 1             # Whether weights must sum to 1
                ,bounded_below_by = 0     # Lower bound for weights, default is 0 (non-negative)
                ,bounded_above_by = 1     # Upper bound for weights, default is 1
                ,princ_comp_count = NULL  # Number of principal components to use, NULL means no dimension reduction
                ,normchoice = c('l1', 'l2')[2] # Choice of norm, default is L2
                ,penalty_normchoice = c('l1', 'l2')[1] # Choice of penalty norm, default is L1
                ,penalty_lambda = 0       # Regularization parameter, default 0 means no regularization
                ,Y = NULL                 # Optional additional time series data
                ,Y_lookback_indices = list(seq(1,1,1)) # Lag structure indices for Y
                ,X_lookback_indices = rep(list(c(1)),length(dbw_indices)) # Lag structure indices for X
                ,inputted_transformation   # Transformation function to apply to Y (e.g., mean_square_y or id)
) { # Reference: https://github.com/DEck13/synthetic_prediction/blob/master/prevalence_testing/numerical_studies/COP.R
  
  # Calculate size of donor pool (total time series minus 1 for target)
  n <- length(X) - 1
  
  # Extract number from normchoice string ('l1'->1, 'l2'->2) for norm function
  normchoice_number <- unlist(strsplit(normchoice, split = ""))[2]

  # Check if model is overparameterized: number of variables p > number of donors n
  p <- length(dbw_indices)
  print('Here is the number of covariates used in dbw:') # Print number of covariates used
  print(p)
  if (p > n){cat('p > n, i.e. system is overdetermined from an unconstrained point-of-view.')} # Warning about overparameterization

  ## Now perform the complex task of extracting specified lag structures
  ## for each time series i=1,2,...,n+1 and each covariate

  # Define function to extract specified columns from dataframes
  col_returner <- function(df){return(df[,dbw_indices])}
  print('col_returner succeeded')

  # Apply col_returner to all time series X to extract covariates of interest
  X_subset1 <- lapply(X, col_returner)
  print('col_returner with lapply succeeded')

  # If Y lookback indices are provided, process Y data and merge with X data
  if (is.null(Y_lookback_indices) == FALSE){

    print('User has provided Y_lookback_indices, so we include them.')
    X_lookback_indices <- c(Y_lookback_indices, X_lookback_indices) # Combine Y and X lookback indices

    # Define function to transform Y and merge with X
    X_Y_combiner <- function(y,x) {
          # Print transformation function info for debugging
          print('We print the transformation and its class')
          print(inputted_transformation)
          print(class(inputted_transformation))

          # Apply specified transformation to y (like mean_square_y or id)
          transformed_series <- inputted_transformation(y)

          # Merge transformed y with x data, all=FALSE means keep only common timepoints
          return(merge(transformed_series,x, all = FALSE))
    } #end X_Y_combiner

    # Print class and length info for Y and X data
    print('We are about to combine X_subset1 and Y...')
    print(class(Y))
    print(length(Y))
    print(class(X_subset1))
    print(length(X_subset1))

    # Apply X_Y_combiner to Y and X data in parallel using mapply
    combined_X <- mapply(X_Y_combiner
                         , y = Y
                         , x = X_subset1
                         , SIMPLIFY = FALSE)
  }
  else{
    # If no Y data, just use X_subset1
    combined_X <- X_subset1
  }

  # Define function to extract data up to shock time for each series
  row_returner <- function(df, stv){
    print(paste('Shock occurs at ', stv, sep = ''))
    print(paste('Row count of the series is ', nrow(df), sep = ''))
    return(df[1:(stv),]) # Return all rows from 1 to shock time
  }

  # Apply row_returner to all merged data, and using the simplify to make it a list
  X_subset2 <- mapply(row_returner, df = combined_X, stv = shock_time_vec, SIMPLIFY=FALSE)

  # Define function to extract covariate values according to specified lag indices
  cov_extractor <- function(X_df){
        # Get row count of dataframe
        len <- nrow(X_df)

        # Define inner function to create boolean vector with TRUE at specified indices
        padded_vector_maker <- function(x)
            {
            vec <- rep(FALSE,len) # Create all-FALSE vector
            vec[x] <- TRUE        # Set specified indices to TRUE
            vec_reversed <- rev(vec) # Reverse vector (count from end)
            return(vec_reversed)
            }

        # Apply padded_vector_maker to each lag index to create list of boolean vectors
        covariate_vals_in_list <- lapply(X_lookback_indices, padded_vector_maker)
        # Convert list of boolean vectors to matrix
        boolmat <- as.matrix(do.call(data.frame, covariate_vals_in_list))

        # Extract specified values from dataframe using boolean matrix
        return(as.matrix(X_df)[boolmat])
  }

  # Apply cov_extractor to all data
  X_subset <- lapply(X_subset2, cov_extractor)

  # Bind all processed data into a single matrix
  dat <- do.call('rbind', X_subset)
  print('Pre-scaling') # Print data before scaling
  print(dat)

  # Apply scaling and centering according to parameters
  if (scale == TRUE) {cat('User has chosen to scale covariates.')}
  if (center == TRUE) {cat('User has chosen to center covariates.')}

  # Apply scale function to each column for standardization and centering
  dat <- apply(dat, 2, function(x) scale(x, center = center, scale = scale))
  print('Post-scaling (nothing will happen if center and scale are set to FALSE).')
  print(dat) # Print processed data

  # Compute singular value decomposition for principal component analysis
  dat.svd <- svd(dat)
  sing_vals <- dat.svd$d / sum(dat.svd$d) # Calculate proportion of variance explained
  print('Singular value percentages for the donor pool X data:')
  print(paste(round(100 * sing_vals,2), "%", sep = "")) # Print singular value percentages

  # If principal component count specified, perform dimension reduction
  if (is.null(princ_comp_count) == FALSE){
    print(paste('We are using ', princ_comp_count, ' principal components.', sep = ''))
    print(dat.svd$v) # Print right singular vectors (principal component directions)
    # Project data onto first princ_comp_count principal components
    dat <- dat %*% dat.svd$v[,1:princ_comp_count]
  }

  # Separate data into target series and donor series
  X1 <- dat[1, , drop = FALSE] # Target series (first row)
  X0 <- split(dat[-1,], seq(nrow(dat[-1,]))) # Donor series (remaining rows), split into list

  print('We inspect X1.') # Print target series
  print(X1)
  print('We inspect X0.') # Print donor series
  print(X0)

  # Define objective function: distance between weighted combination of donors and target
  weightedX0 <- function(W) {
    # W is weight vector with length equal to X0's length
    n <- length(W)
    p <- ncol(X1)
    XW <- matrix(0, nrow = 1, ncol = p) # Initialize weighted combination matrix

    # Calculate weighted sum
    for (i in 1:n) {
      XW <- XW + W[i] * X0[[i]]
    } #end of loop

    # Calculate distance between target and weighted combination using specified norm
    norm_output <- as.numeric(norm(matrix(X1 - XW), type = normchoice_number))

    # Add regularization penalty
    if (penalty_normchoice == 'l1' & penalty_lambda > 0) {
      # L1 regularization (like LASSO)
      norm_output <- norm_output + penalty_lambda * norm(as.matrix(W), type = "1")
    }
    else if (penalty_normchoice == 'l2' & penalty_lambda > 0) {
      # L2 regularization (like Ridge)
      norm_output <- norm_output + penalty_lambda * as.numeric(crossprod(matrix(W)))
    }
    else {norm_output <- norm_output} # No regularization

    return(norm_output)
  } #end objective function

  # Optimization to find optimal weights

  # The function has been enhanced with features:
  # 1) Option to remove sum-to-1 constraint
  # 2) Option to change lower bound to -1 or NA (no limit)
  # 3) Option to change upper bound to NA (no limit)
  # 4) Option to choose L1 or L2 norm as distance function

  # Set up constraints according to parameters

  # Set up sum-to-1 constraint
  if (is.na(sum_to_1) == FALSE) {
    eq_constraint <- function(W) sum(W) - 1 # Sum of weights minus 1
  }
  else{
    eq_constraint = NULL # No sum-to-1 constraint
  }

  # Set up lower bound constraint
  if (is.na(bounded_below_by) == FALSE) {
    lower_bound = rep(bounded_below_by, n) # Same lower bound for all weights
  }
  else if (is.na(bounded_below_by) == TRUE) {
    lower_bound = NULL # No lower bound
  }

  # Set up upper bound constraint
  if (is.na(bounded_above_by) == FALSE) {
    upper_bound = rep(bounded_above_by, n) # Same upper bound for all weights
  }
  else if (is.na(bounded_above_by) == TRUE) {
    upper_bound = NULL # No upper bound
  }

  # Solve constrained optimization problem using Rsolnp package
  object_to_return <- Rsolnp::solnp(
                            par = rep(1/n, n),        # Initial values: equal weights
                            fun = weightedX0,         # Objective function
                            eqfun = eq_constraint,    # Equality constraint function
                            eqB = 0,                 # Right-hand side of equality constraint
                            LB = lower_bound, UB = upper_bound, # Bounds
                            control = list(trace = 1  # Output optimization process
                                           , 1.0e-12  # Control parameter
                                           , tol = 1e-27 # Tolerance
                                           , outer.iter = 1000000000 # Max outer iterations
                                           , inner.iter = 10000000)) # Max inner iterations

  # Calculate final loss value
  loss <- round(norm(X1 - object_to_return$pars %*% dat[-1,], type = normchoice_number),3)

  # Print loss information
  print(paste('The loss of distanced-based weighting is ', loss, ',',
              ' which is ',
              100*round(loss/norm(X1, type=normchoice_number),3),
              "% of the norm of the vector we are trying to approximate.", sep = ""))

  # Check if optimization converged
  if (object_to_return$convergence == 0){
    convergence <- 'convergence' # Convergence successful
  }
  else {
    convergence <- 'failed_convergence' # Convergence failed
  }

  # Create return result list
  pair_to_return <- list(object_to_return$pars, convergence)

  # Add names to result list
  names(pair_to_return) <- c('opt_params', 'convergence')

  # Return optimal weights and convergence status
  return(pair_to_return)

} #END dbw function
### END dbw
```

## plot function may be used in the arime part
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START GARCH plot_maker_garch
#only accept the volatility
plot_maker_garch <- function(fitted_vol
                            ,shock_time_labels = NULL
                            ,shock_time_vec #mk
                            ,shock_length_vec
                            ,unadjusted_pred
                            ,w_hat
                            #,omega_star_hat(this variable is not used)erdf 
                            ,omega_star_hat_vec
                            ,omega_star_std_err_hat_vec
                            ,adjusted_pred
                            ,arithmetic_mean_based_pred
                            ,ground_truth_vec){

  if (is.character(shock_time_labels) == FALSE | is.null(shock_time_labels) == TRUE){
    shock_time_labels <- 1:length(shock_time_vec)
  }

  par(mfrow = c(1,3), mar=c(15,9,4,2))

  barplot_colors <- RColorBrewer::brewer.pal(length(w_hat),'Set3')

  #PLOT ON THE LEFT:
print('We plot the weights.')
  # Plot donor weights
  barplot(w_hat
          , main = 'Donor Pool Weights'
          , names.arg = shock_time_labels[-1]
          , cex.names=1.3
          , cex.main=1.5
          , las=2
          , col = barplot_colors
          )

  #PLOT IN THE MIDDLE
  print('We plot the FE estimates.')
  #Plot FE estimates
    bp <- barplot(omega_star_hat_vec
          , main = 'Donor-Pool-Supplied\n FE Estimates\nand Standard Errors Estimates'
          , names.arg = shock_time_labels[-1]
          , cex.names=1.4
          , cex.main=1.5
          , las=2
          , col = barplot_colors
          , ylim = c(0, 1.4 * max(omega_star_hat_vec)) )

    # Add the labels with some offset to be above the bar
    print('We print the std errors')
    print(omega_star_std_err_hat_vec)
    #omega_star_std_err_hat_vec <- ifelse(omega_star_std_err_hat_vec, nan)

    #https://stackoverflow.com/questions/65057352/how-to-add-labels-above-the-bar-of-barplot-graphics
    text(x = bp,
                ,y = omega_star_hat_vec + .00029
                ,cex = 1.3
                ,labels = round(omega_star_std_err_hat_vec, 5)
                , srt= 90)

  title(ylab = expression(sigma^2), line = 3.05, cex.lab = 1.99) # Add y-axis text

  #Plot target series and prediction

  thing_to_get_max_of <- c(as.numeric(fitted_vol)
                        , unadjusted_pred
                        , adjusted_pred
                        , ground_truth_vec
                        , arithmetic_mean_based_pred
                        )

  max_for_y_lim <- max(thing_to_get_max_of)

  x_ax_first_point_of_shock <- index(fitted_vol)[shock_time_vec[1]-1] + 1 #do I use this?
  x_ax_end_point <- index(fitted_vol)[shock_time_vec[1]-1] + length(adjusted_pred)

  #PLOT ON THE RIGHT:
  print('We plot the fitted volatility series.')
  plot(y = fitted_vol[1:shock_time_vec[1]], #mk
          x = index(fitted_vol)[1:shock_time_vec[1]],
       main = 'Post-Shock Volatility Forecast', #mk can improve this title
       cex.main=1.5,
       ylab = '',
       type="l",
       xlab = '',
       xlim =  as.Date(c(index(fitted_vol)[1], x_ax_end_point)),
       ylim = c(min(0, fitted_vol),  max_for_y_lim))

  title(ylab = expression(sigma^2), line = 2.05, cex.lab = 1.99) # Add y-axis text

  # Here is the color scheme we will use
  # https://colorbrewer2.org/?type=diverging&scheme=RdYlBu&n=4
  colors_for_adjusted_pred <- c('#d7191c','#fdae61','#abd9e9','#2c7bb6')

  # Let's add the plain old GARCH prediction
  points(y = unadjusted_pred
         ,x = x_ax_first_point_of_shock:x_ax_end_point
         ,col = colors_for_adjusted_pred[1]
         ,cex = 3.5
         ,pch = 15)

  # Now plot the adjusted predictions
  points(y = adjusted_pred
         ,x = x_ax_first_point_of_shock:x_ax_end_point
         ,col = colors_for_adjusted_pred[2]
         ,cex = 3.5
         ,pch = 18)

  # Now plot the arithmetic mean-based predictions
  points(y = arithmetic_mean_based_pred
         ,x = x_ax_first_point_of_shock:x_ax_end_point
         ,col = colors_for_adjusted_pred[3]
         ,cex = 3.5
         ,pch = 19)

  # Now plot Ground Truth tk
  if (is.null(ground_truth_vec) == FALSE)
    {
    points(y = ground_truth_vec
           ,x = x_ax_first_point_of_shock:x_ax_end_point
           ,col = colors_for_adjusted_pred[4]
           ,cex = 3.5
           ,pch = 17)
  }

  labels_for_legend <- c('GARCH (unadjusted)'
                        , 'Adjusted'
                        , 'Arithmetic Mean'
                        , 'Ground Truth'
                        )

  legend(x = "topleft",  # Coordinates (x also accepts keywords) #mk
         legend = labels_for_legend,
         1:length(labels_for_legend), # Vector with the name of each group
         colors_for_adjusted_pred,   # Creates boxes in the legend with the specified colors
         title = 'Prediction Method',      # Legend title,
         cex = .9)

  #par(mfrow = c(1,1), mar=c(15,6,4,2))

}
### END plot_maker_garch
```
## Starting the arime part testing
### Function4: SynthPrediction
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### SynthPrediction Function
### Purpose: Synthetic Prediction for Time Series with Shock Analysis
SynthPrediction <- function(Y_series_list
                            ,covariates_series_list
                            ,shock_time_vec
                            ,shock_length_vec
                            ,k = 1
                            ,dbw_scale = TRUE
                            ,dbw_center = TRUE
                            ,dbw_indices = NULL
                            ,princ_comp_input = min(length(shock_time_vec), ncol(covariates_series_list[[1]]))
                            ,covariate_indices = NULL
                            ,geometric_sets = NULL
                            ,days_before_shocktime_vec = NULL
                            ,arima_order = NULL
                            ,seasonal = FALSE
                            ,seasonal_order = NULL
                            ,seasonal_period = NULL
                            ,user_ic_choice = c('aicc','aic','bic')[1]
                            ,stepwise = TRUE
                            ,approximation = TRUE
                            ,plots = TRUE
                            ,display_ground_truth_choice = FALSE
) {
  ### Function Overview:
  # (Overview comments retained...)

  n <- length(Y_series_list) - 1
  if (is.null(arima_order)) arima_order <- c(1,1,1)
  if (is.null(dbw_indices)) dbw_indices <- 1:ncol(covariates_series_list[[1]])

  integer_shock_time_vec <- c()
  integer_shock_time_vec_for_convex_hull_based_optimization <- c()
  for (i in 1:(n+1)){
    if (is.character(shock_time_vec[i])) {
      integer_shock_time_vec[i] <- which(index(Y_series_list[[i]]) == shock_time_vec[i])
      integer_shock_time_vec_for_convex_hull_based_optimization[i] <- which(index(covariates_series_list[[i]]) == shock_time_vec[i])
    } else {
      integer_shock_time_vec[i] <- shock_time_vec[i]
      integer_shock_time_vec_for_convex_hull_based_optimization[i] <- shock_time_vec[i]
    }
  }

  omega_star_hat_vec <- c()
  order_of_arima <- list()
  for (i in 2:(n+1)) {
    vec_of_zeros <- rep(0, integer_shock_time_vec[i])
    vec_of_ones <- rep(1, shock_length_vec[i])
    post_shock_indicator <- c(vec_of_zeros, vec_of_ones)
    last_shock_point <- integer_shock_time_vec[i] + shock_length_vec[i]

    if (is.null(covariate_indices)) {
      X_i_penultimate <- cbind(Y_series_list[[i]][1:last_shock_point], post_shock_indicator)
      X_i_final <- X_i_penultimate[,2]
    } else {
      X_i_subset <- covariates_series_list[[i]][1:last_shock_point,covariate_indices]
      X_i_final <- cbind(X_i_subset, post_shock_indicator)
    }

    # —— Fit (S)ARIMA model to donor series i —— 

# 1) Determine seasonal period for this donor
seasonal_period_i <- seasonal_period
if (isTRUE(seasonal) && is.null(seasonal_period_i)) {
  seasonal_period_i <- tryCatch({
    forecast::findfrequency(
      as.numeric(Y_series_list[[i]][1:last_shock_point])
    )
  }, error = function(e) {
    warning(sprintf(
      "Donor %d: seasonal auto-detect failed → default = 4", i
    ))
    4
  })
}
# 2) Clean up any NAs in X_i_final (from lagging)
if (!is.null(covariate_indices)) {
  X_i_final <- na.omit(X_i_final)
}
# 3) Fit model: user‐specified order or auto.arima()
if (!is.null(arima_order)) {
  donor_model <- forecast::Arima(
    y        = Y_series_list[[i]][1:last_shock_point],
    order    = arima_order,
    seasonal = if (!is.null(seasonal_order))
                  list(order  = seasonal_order,
                       period = seasonal_period_i)
               else FALSE,
    xreg     = X_i_final
  )
} else {
  donor_model <- tryCatch(
    forecast::auto.arima(
      y             = Y_series_list[[i]][1:last_shock_point],
      xreg          = X_i_final,
      ic            = user_ic_choice,
      seasonal      = seasonal,
      period        = seasonal_period_i,
      stepwise      = stepwise,
      approximation = approximation
    ),
    error = function(e) {
      warning(sprintf(
        "Donor %d: auto.arima() failed → falling back to Arima(1,1,1)", i
      ))
      forecast::Arima(
        y     = Y_series_list[[i]][1:last_shock_point],
        order = c(1,1,1),
        xreg  = X_i_final
      )
    }
  )
}

# 4) Record model order and extract shock‐indicator coefficient
order_of_arima[[i]]           <- donor_model$arma
coef_test                     <- lmtest::coeftest(donor_model)
extracted_fixed_effect        <- coef_test[nrow(coef_test), "Estimate"]
omega_star_hat_vec[i - 1]     <- extracted_fixed_effect

  }

  dbw_output <- dbw(
    covariates_series_list, dbw_indices, integer_shock_time_vec,
    scale = TRUE, center = TRUE, sum_to_1 = TRUE,
    bounded_below_by = 0, bounded_above_by = 1,
    Y = Y_series_list, inputted_transformation = id
  )

  w_hat <- dbw_output[[1]]
  omega_star_hat <- as.numeric(w_hat %*% omega_star_hat_vec)

  ### Forecast Target Series
target_series <- Y_series_list[[1]][1:(integer_shock_time_vec[1])]
forecast_horizon <- shock_length_vec[1]

# Prepare covariates
if (is.null(covariate_indices)) {
  xreg_input <- NULL
  xreg_future <- NULL
} else {
  X_lagged <- lag.xts(covariates_series_list[[1]][1:integer_shock_time_vec[1], covariate_indices])
  xreg_input <- X_lagged
  X_last <- covariates_series_list[[1]][integer_shock_time_vec[1], covariate_indices]
  xreg_future <- matrix(rep(X_last, forecast_horizon), nrow = forecast_horizon, byrow = TRUE)
}

# Detect seasonal period if needed
target_seasonal_period <- seasonal_period
if (isTRUE(seasonal) && is.null(target_seasonal_period)) {
  target_seasonal_period <- tryCatch({
    forecast::findfrequency(as.numeric(target_series))
  }, error = function(e) {
    warning("Target: seasonal period auto-detection failed — using default = 4")
    4
  })
}

# Fit model
if (!is.null(arima_order)) {
  target_model <- forecast::Arima(
    y        = target_series,
    order    = arima_order,
    seasonal = list(order = seasonal_order, period = target_seasonal_period),
    xreg     = xreg_input
  )
} else {
  target_model <- forecast::auto.arima(
    y             = target_series,
    xreg          = xreg_input,
    ic            = user_ic_choice,
    seasonal      = seasonal,
    period        = target_seasonal_period,
    stepwise      = stepwise,
    approximation = approximation
  )
}

# Forecast
if (is.null(xreg_future)) {
  unadjusted_pred <- predict(target_model, n.ahead = forecast_horizon)
} else {
  unadjusted_pred <- predict(target_model, n.ahead = forecast_horizon, newxreg = xreg_future)
}

# Adjusted forecast
adjusted_pred <- unadjusted_pred$pred + omega_star_hat


  list_of_linear_combinations <- list(w_hat)
  list_of_forecasts <- list(unadjusted_pred, adjusted_pred)
  names(list_of_forecasts) <- c('unadjusted_pred', 'adjusted_pred')
  output_list <- list(list_of_linear_combinations, list_of_forecasts)
  names(output_list) <- c('linear_combinations', 'predictions')

  if (plots == TRUE){
    plot_maker_synthprediction(Y_series_list,
                               shock_time_vec,
                               integer_shock_time_vec,
                               shock_length_vec,
                               unadjusted_pred$pred,
                               w_hat,
                               omega_star_hat,
                               omega_star_hat_vec,
                               adjusted_pred,
                               display_ground_truth = display_ground_truth_choice)
  }

  return(output_list)
}
```

## SynthPrediction Function — Key Enhancements

Compared to the original, single-path implementation, this version adds:

1. **User-specified ARIMA order**  
   - Introduces `arima_order`. When non-`NULL`, the function calls `forecast::Arima()` with the user’s `(p,d,q)` (and optionally `(P,D,Q)`), bypassing the default `auto.arima()` search.

2. **SARIMA (seasonal) support**  
   - Adds `seasonal`, `seasonal_order` and `seasonal_period` parameters.  
   - If `seasonal = TRUE` and no period is provided, automatically detects frequency via `forecast::findfrequency()`.  
   - Honors user-supplied seasonal orders when fitting with `Arima()`.

3. **Flexible auto-search controls**  
   - Exposes `stepwise` and `approximation` arguments in the `auto.arima()` call, letting users trade off speed vs. accuracy.  
   - Wraps the call in `tryCatch()` so that any fit failure falls back to a simple `Arima(1,1,1)` rather than crashing.

4. **Robust covariate lag & NA handling**  
   - After creating lagged regressors (`lag.xts()`), automatically drops leading `NA` rows via `na.omit()`, guaranteeing that the dependent series and `xreg` remain time-aligned.

5. **Character or numeric shock times**  
   - Accepts both date strings and index positions in `shock_time_vec`, converting either form into integer indices without manual user intervention.

6. **Fixed-effect extraction from donors**  
   - Uses `lmtest::coeftest()` on each donor’s fit to pull out the coefficient on the “post_shock_indicator” regressor, storing these in `omega_star_hat_vec`.

7. **Distance-based weighting integration**  
   - Calls the custom `dbw()` function to compute optimal donor weights `w_hat`, then forms the aggregate shock effect `omega_star_hat = w_hat %*% omega_star_hat_vec`.

8. **Unified multi-step forecasting & adjustment**  
   - Fits the target series with the same ARIMA/SARIMA logic.  
   - Generates an unadjusted forecast (via `predict()` or `forecast::forecast()`), then simply adds `omega_star_hat` to produce the “adjusted” forecast.

9. **Structured output and plotting**  
   - Returns a list containing both the donor weight vector and two forecast objects (`unadjusted_pred`, `adjusted_pred`).  
   - Includes a `plots` flag to trigger `plot_maker_synthprediction()`, producing side-by-side time-series comparisons.

10. **Verbose console feedback**  
    - Prints donor count, shock times, weights, MSE, and forecast comparisons via `cat()`, aiding interactive debugging and auditability.

## Next Steps

- **This week**: Test the function thoroughly using simulated data to ensure all branches work correctly.  
- **Next week**: If you have time, I’d like to schedule a meeting to review results and discuss the plan for further development.
