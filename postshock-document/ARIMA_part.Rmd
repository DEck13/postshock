---
title: "Postshock"
author: "Qiyang Wang"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 4
urlcolor: blue
---
## Nike data
Import the data form NIKE
```{r}
nike<-read.csv("NKE_quarterly_valuation_measures.csv",header = TRUE)
nike1<-nike[,-c(1,2)]
A<-matrix(NA,nrow=ncol(nike1),ncol=3)
for (i in 1:ncol(nike1)) {
  A[i,1]<-colnames(nike1)[i]
  A[i,2]<-nike1[2,i]
  A[i,3]<-nike1[8,i]
  
}
A<-as.data.frame(A)
colnames(A)<-c("date","ev","ev.rev")
A$ev.rev<-as.numeric(A$ev.rev)
A$ev<-as.numeric(gsub(",","",A$ev))
A$date<-gsub("X","",A$date)
A$revenue<-A$ev/A$ev.rev
b<-c()
i=1
for (i in 1:(nrow(A)-1)) {
  b[i]<-((A[i,4]-A[i+1,4])*100/A[i+1,4])/100
}
b[nrow(A)]<-0
A$per<-b
A$per<-round(A$per,3)
Nike.f<-A
Nike.f<-Nike.f[1:which(Nike.f$date=="02.28.2001", arr.ind=TRUE),]
```

## import the dbw and QL loss function
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START QL_loss_function
QL_loss_function <- function(pred, gt){gt/pred - log(gt/pred) - 1}
### END QL_loss_function

#We specify some functions for transforming y

#mean_square_y will be used in garch models
mean_square_y <- function(y){return((y-mean(y))**2)}

#identity function will be used in synthetic prediction models
id <- function(y){return(y)}

```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START dbw
dbw <- function(X,                        # List of covariate time series (X[[1]] = target, X[[2]]~X[[n+1]] = donors)
                dbw_indices,             # Indices of covariate columns to use (e.g., 1, 1:3, etc.)
                shock_time_vec,          # Vector of shock time indices (one per series)
                scale = FALSE,           # Whether to standardize covariate columns
                center = FALSE,          # Whether to mean-center covariate columns
                sum_to_1 = 1,            # Whether weights must sum to 1
                bounded_below_by = 0,    # Lower bound for weights (e.g., 0 for non-negativity)
                bounded_above_by = 1,    # Upper bound for weights (e.g., 1 for simplex)
                princ_comp_count = NULL, # Number of principal components to use (NULL = no reduction)
                normchoice = 'l2',       # Distance metric to minimize ('l1' or 'l2')
                penalty_normchoice = 'l1', # Norm for regularization penalty ('l1' = LASSO, 'l2' = Ridge)
                penalty_lambda = 0,      # Regularization strength
                Y = NULL,                # Optional Y series for transformation-based augmentation
                Y_lookback_indices = list(seq(1, 1, 1)), # Lags to use for Y if given
                X_lookback_indices = rep(list(c(1)), length(dbw_indices)), # Lags to use for each X covariate
                inputted_transformation  # Transformation function applied to Y (e.g., id or mean_square_y)
) {
  n <- length(X) - 1  # Number of donors
  p <- length(dbw_indices)
  normchoice_number <- unlist(strsplit(normchoice, split = ""))[2]

  # === Safety check: Make sure all X[[i]] have enough columns for dbw_indices ===
  for (i in seq_along(X)) {
    if (ncol(X[[i]]) < max(dbw_indices)) {
      stop(paste0("X[[", i, "]] has only ", ncol(X[[i]]),
                  " column(s), but dbw_indices = ", paste(dbw_indices, collapse = ",")))
    }
  }

  # === Extract selected covariate columns ===
  col_returner <- function(df) {
    return(df[, dbw_indices, drop = FALSE])
  }
  X_subset1 <- lapply(X, col_returner)

  # === Merge Y and X if Y is provided ===
  if (!is.null(Y_lookback_indices)) {
    X_lookback_indices <- c(Y_lookback_indices, X_lookback_indices)
    X_Y_combiner <- function(y, x) {
      transformed_y <- inputted_transformation(y)
      return(merge(transformed_y, x, all = FALSE))
    }
    combined_X <- mapply(X_Y_combiner, y = Y, x = X_subset1, SIMPLIFY = FALSE)
  } else {
    combined_X <- X_subset1
  }

  # === Trim all series up to the shock time ===
  row_returner <- function(df, stv) {
    return(df[1:stv, , drop = FALSE])
  }
  X_subset2 <- mapply(row_returner, df = combined_X, stv = shock_time_vec, SIMPLIFY = FALSE)

  # === Extract specified lagged rows for each covariate ===
  cov_extractor <- function(X_df) {
    len <- nrow(X_df)
    padded_vector_maker <- function(x) {
      vec <- rep(FALSE, len)
      vec[x] <- TRUE
      rev(vec)
    }
    boolmat <- as.matrix(do.call(data.frame, lapply(X_lookback_indices, padded_vector_maker)))
    vals <- as.matrix(X_df)[boolmat]
    return(matrix(vals, nrow = 1))  # Always return as 1-row matrix
  }
  X_subset <- lapply(X_subset2, cov_extractor)

  # === Combine all rows into matrix ===
  dat <- do.call('rbind', X_subset)

  # === Check for invalid data matrix ===
  if (nrow(dat) == 0 || ncol(dat) == 0) {
    stop("Covariate matrix 'dat' is empty. Check dbw_indices or lag settings.")
  }

  # === Standardize or center covariates if specified ===
  if (scale || center) {
    dat <- apply(dat, 2, function(x) scale(x, center = center, scale = scale))
  }

  # === PCA reduction (optional) ===
  dat.svd <- svd(dat)
  if (!is.null(princ_comp_count)) {
    dat <- dat %*% dat.svd$v[, 1:princ_comp_count, drop = FALSE]
  }

  # === Split into target and donors ===
  X1 <- dat[1, , drop = FALSE]
  dat_donors <- dat[-1, , drop = FALSE]
  X0 <- split(dat_donors, seq(nrow(dat_donors)))

  # === Define objective function ===
  weightedX0 <- function(W) {
    XW <- Reduce(`+`, Map(`*`, W, X0))
    loss <- as.numeric(norm(X1 - XW, type = normchoice_number))
    if (penalty_lambda > 0) {
      if (penalty_normchoice == 'l1') {
        loss <- loss + penalty_lambda * sum(abs(W))
      } else if (penalty_normchoice == 'l2') {
        loss <- loss + penalty_lambda * sqrt(sum(W^2))
      }
    }
    return(loss)
  }

  # === Constraints for optimization ===
  eq_constraint <- if (!is.na(sum_to_1)) function(W) sum(W) - 1 else NULL
  lower_bound <- if (!is.na(bounded_below_by)) rep(bounded_below_by, n) else NULL
  upper_bound <- if (!is.na(bounded_above_by)) rep(bounded_above_by, n) else NULL

  # === Optimization using Rsolnp ===
  object_to_return <- Rsolnp::solnp(
    par = rep(1/n, n),
    fun = weightedX0,
    eqfun = eq_constraint,
    eqB = 0,
    LB = lower_bound,
    UB = upper_bound,
    control = list(trace = 1, tol = 1e-8)
  )

  # === Compute approximation loss (optional) ===
  loss <- round(norm(X1 - object_to_return$pars %*% dat_donors, type = normchoice_number), 3)
  convergence <- if (object_to_return$convergence == 0) "convergence" else "failed_convergence"

  # === Return optimal weights and status ===
  return(list(opt_params = object_to_return$pars,
              convergence = convergence,
              loss = loss))
}
### END dbw

```

## plot function may be used in the arime part
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START GARCH plot_maker_garch
#only accept the volatility
plot_maker_garch <- function(fitted_vol
                            ,shock_time_labels = NULL
                            ,shock_time_vec #mk
                            ,shock_length_vec
                            ,unadjusted_pred
                            ,w_hat
                            #,omega_star_hat(this variable is not used)erdf 
                            ,omega_star_hat_vec
                            ,omega_star_std_err_hat_vec
                            ,adjusted_pred
                            ,arithmetic_mean_based_pred
                            ,ground_truth_vec){

  if (is.character(shock_time_labels) == FALSE | is.null(shock_time_labels) == TRUE){
    shock_time_labels <- 1:length(shock_time_vec)
  }

  par(mfrow = c(1,3), mar=c(15,9,4,2))

  barplot_colors <- RColorBrewer::brewer.pal(length(w_hat),'Set3')

  #PLOT ON THE LEFT:
print('We plot the weights.')
  # Plot donor weights
  barplot(w_hat
          , main = 'Donor Pool Weights'
          , names.arg = shock_time_labels[-1]
          , cex.names=1.3
          , cex.main=1.5
          , las=2
          , col = barplot_colors
          )

  #PLOT IN THE MIDDLE
  print('We plot the FE estimates.')
  #Plot FE estimates
    bp <- barplot(omega_star_hat_vec
          , main = 'Donor-Pool-Supplied\n FE Estimates\nand Standard Errors Estimates'
          , names.arg = shock_time_labels[-1]
          , cex.names=1.4
          , cex.main=1.5
          , las=2
          , col = barplot_colors
          , ylim = c(0, 1.4 * max(omega_star_hat_vec)) )

    # Add the labels with some offset to be above the bar
    print('We print the std errors')
    print(omega_star_std_err_hat_vec)
    #omega_star_std_err_hat_vec <- ifelse(omega_star_std_err_hat_vec, nan)

    #https://stackoverflow.com/questions/65057352/how-to-add-labels-above-the-bar-of-barplot-graphics
    text(x = bp,
                ,y = omega_star_hat_vec + .00029
                ,cex = 1.3
                ,labels = round(omega_star_std_err_hat_vec, 5)
                , srt= 90)

  title(ylab = expression(sigma^2), line = 3.05, cex.lab = 1.99) # Add y-axis text

  #Plot target series and prediction

  thing_to_get_max_of <- c(as.numeric(fitted_vol)
                        , unadjusted_pred
                        , adjusted_pred
                        , ground_truth_vec
                        , arithmetic_mean_based_pred
                        )

  max_for_y_lim <- max(thing_to_get_max_of)

  x_ax_first_point_of_shock <- index(fitted_vol)[shock_time_vec[1]-1] + 1 #do I use this?
  x_ax_end_point <- index(fitted_vol)[shock_time_vec[1]-1] + length(adjusted_pred)

  #PLOT ON THE RIGHT:
  print('We plot the fitted volatility series.')
  plot(y = fitted_vol[1:shock_time_vec[1]], #mk
          x = index(fitted_vol)[1:shock_time_vec[1]],
       main = 'Post-Shock Volatility Forecast', #mk can improve this title
       cex.main=1.5,
       ylab = '',
       type="l",
       xlab = '',
       xlim =  as.Date(c(index(fitted_vol)[1], x_ax_end_point)),
       ylim = c(min(0, fitted_vol),  max_for_y_lim))

  title(ylab = expression(sigma^2), line = 2.05, cex.lab = 1.99) # Add y-axis text

  # Here is the color scheme we will use
  # https://colorbrewer2.org/?type=diverging&scheme=RdYlBu&n=4
  colors_for_adjusted_pred <- c('#d7191c','#fdae61','#abd9e9','#2c7bb6')

  # Let's add the plain old GARCH prediction
  points(y = unadjusted_pred
         ,x = x_ax_first_point_of_shock:x_ax_end_point
         ,col = colors_for_adjusted_pred[1]
         ,cex = 3.5
         ,pch = 15)

  # Now plot the adjusted predictions
  points(y = adjusted_pred
         ,x = x_ax_first_point_of_shock:x_ax_end_point
         ,col = colors_for_adjusted_pred[2]
         ,cex = 3.5
         ,pch = 18)

  # Now plot the arithmetic mean-based predictions
  points(y = arithmetic_mean_based_pred
         ,x = x_ax_first_point_of_shock:x_ax_end_point
         ,col = colors_for_adjusted_pred[3]
         ,cex = 3.5
         ,pch = 19)

  # Now plot Ground Truth tk
  if (is.null(ground_truth_vec) == FALSE)
    {
    points(y = ground_truth_vec
           ,x = x_ax_first_point_of_shock:x_ax_end_point
           ,col = colors_for_adjusted_pred[4]
           ,cex = 3.5
           ,pch = 17)
  }

  labels_for_legend <- c('GARCH (unadjusted)'
                        , 'Adjusted'
                        , 'Arithmetic Mean'
                        , 'Ground Truth'
                        )

  legend(x = "topleft",  # Coordinates (x also accepts keywords) #mk
         legend = labels_for_legend,
         1:length(labels_for_legend), # Vector with the name of each group
         colors_for_adjusted_pred,   # Creates boxes in the legend with the specified colors
         title = 'Prediction Method',      # Legend title,
         cex = .9)

  #par(mfrow = c(1,1), mar=c(15,6,4,2))

}
### END plot_maker_garch
```
## Starting the arime part testing
### Function4: SynthPrediction
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### SynthPrediction Function
### Purpose: Synthetic Prediction for Time Series with Shock Analysis
SynthPrediction <- function(Y_series_list,
                            covariates_series_list,
                            shock_time_vec,
                            shock_length_vec,
                            k = 1,
                            dbw_scale = TRUE,
                            dbw_center = TRUE,
                            dbw_indices = NULL,
                            princ_comp_input = min(length(shock_time_vec), ncol(covariates_series_list[[1]])),
                            covariate_indices = NULL,
                            geometric_sets = NULL,
                            days_before_shocktime_vec = NULL,
                            arima_order = NULL,
                            seasonal = FALSE,
                            seasonal_order = NULL,
                            seasonal_period = NULL,
                            user_ic_choice = c('aicc','aic','bic')[1],
                            stepwise = TRUE,
                            approximation = TRUE,
                            plots = TRUE,
                            display_ground_truth_choice = FALSE,
                            penalty_lambda = 0,
                            penalty_normchoice = "l1"
) {
  ### Function Overview:
  n <- length(Y_series_list) - 1
  if (is.null(dbw_indices)) dbw_indices <- 1:ncol(covariates_series_list[[1]])

  integer_shock_time_vec <- c()
  integer_shock_time_vec_for_convex_hull_based_optimization <- c()
  for (i in 1:(n+1)){
    if (is.character(shock_time_vec[i])) {
      integer_shock_time_vec[i] <- which(index(Y_series_list[[i]]) == shock_time_vec[i])
      integer_shock_time_vec_for_convex_hull_based_optimization[i] <- which(index(covariates_series_list[[i]]) == shock_time_vec[i])
    } else {
      integer_shock_time_vec[i] <- shock_time_vec[i]
      integer_shock_time_vec_for_convex_hull_based_optimization[i] <- shock_time_vec[i]
    }
  }

  omega_star_hat_vec <- c()
  order_of_arima <- list()
  for (i in 2:(n+1)) {
    vec_of_zeros <- rep(0, integer_shock_time_vec[i])
    vec_of_ones <- rep(1, shock_length_vec[i])
    post_shock_indicator <- c(vec_of_zeros, vec_of_ones)
    last_shock_point <- integer_shock_time_vec[i] + shock_length_vec[i]

    if (is.null(covariate_indices)) {
      X_i_penultimate <- cbind(Y_series_list[[i]][1:last_shock_point], post_shock_indicator)
      X_i_final <- X_i_penultimate[,2]
    } else {
      X_i_subset <- covariates_series_list[[i]][1:last_shock_point,covariate_indices]
      X_i_final <- cbind(X_i_subset, post_shock_indicator)
    }

    # —— Fit (S)ARIMA model to donor series i —— 

seasonal_period_i <- seasonal_period
  if (isTRUE(seasonal) && is.null(seasonal_period_i)) {
    # Auto‐detect seasonal frequency; if that fails, default to 4
    seasonal_period_i <- tryCatch({
      forecast::findfrequency(
        as.numeric(Y_series_list[[i]][1:last_shock_point])
      )
    }, error = function(e) {
      warning(sprintf(
        "Donor %d: seasonal auto-detect failed → default = 4", i
      ))
      4
    })
  }

  # ——— Remove any NA rows in X_i_final created by lagging ———
  if (!is.null(covariate_indices)) {
    X_i_final <- na.omit(X_i_final)
  }

  # ——— Fit a (Seasonal) ARIMA model for donor i ———
  # Case A: User provided a fixed arima_order
  if (!is.null(arima_order)) {
    if (isTRUE(seasonal) && !is.null(seasonal_order)) {
      # If seasonal = TRUE and a seasonal_order is given, include the seasonal submodel
      donor_model <- forecast::Arima(
        y        = Y_series_list[[i]][1:last_shock_point],
        order    = arima_order,
        seasonal = list(order  = seasonal_order, 
                        period = seasonal_period_i),
        xreg     = X_i_final
      )
    } else {
      # Otherwise, do not include any seasonal component
      donor_model <- forecast::Arima(
        y        = Y_series_list[[i]][1:last_shock_point],
        order    = arima_order,
        seasonal = FALSE,
        xreg     = X_i_final
      )
    }

  # Case B: Let auto.arima() select the best (seasonal) ARIMA specification
  } else {
   donor_model <- tryCatch(
  forecast::auto.arima(
    y        = Y_series_list[[i]][1:last_shock_point],
    xreg     = X_i_final,
    ic       = user_ic_choice,
    seasonal = seasonal,
    stepwise = stepwise,
    approximation = approximation
      ),
      error = function(e) {
        # If auto.arima() fails, fall back to a simple ARIMA(1,1,1) with no seasonal
        warning(sprintf(
          "Donor %d: auto.arima() failed → fallback to Arima(1,1,1)", i
        ))
        forecast::Arima(
          y        = Y_series_list[[i]][1:last_shock_point],
          order    = c(1,1,1),
          seasonal = FALSE,
          xreg     = X_i_final
        )
      }
    )
  }

  # Record the chosen ARIMA parameters and extract the shock coefficient
  order_of_arima[[i]] <- donor_model$arma
  coef_test           <- lmtest::coeftest(donor_model)
  # The last row of coef_test corresponds to the “post_shock_indicator” coefficient
  extracted_fixed_effect <- coef_test[nrow(coef_test), "Estimate"]
  omega_star_hat_vec[i - 1] <- extracted_fixed_effect
}

if (is.null(covariate_indices)) {
  n_donors <- length(Y_series_list) - 1
  w_hat <- rep(1 / n_donors, n_donors)
  omega_star_hat <- sum(w_hat * omega_star_hat_vec)

} else {
  dbw_output <- dbw(
    X = covariates_series_list,
    dbw_indices = dbw_indices,
    shock_time_vec = integer_shock_time_vec,
    scale = TRUE,
    center = TRUE,
    sum_to_1 = TRUE,
    bounded_below_by = 0,
    bounded_above_by = 1,
    Y = Y_series_list,
    Y_lookback_indices = NULL,
    X_lookback_indices = rep(list(1), length(dbw_indices)),
    inputted_transformation = id,
    penalty_lambda = penalty_lambda,
    penalty_normchoice = penalty_normchoice
  )

  w_hat <- dbw_output$opt_params
  omega_star_hat <- as.numeric(w_hat %*% omega_star_hat_vec)
}

  ### Forecast Target Series
target_series <- Y_series_list[[1]][1:(integer_shock_time_vec[1])]
forecast_horizon <- k

# Construct covariate matrices for training and forecasting, if applicable
if (is.null(covariate_indices)) {
  xreg_input  <- NULL
  xreg_future <- NULL
} else {
  # Create lagged training covariate values (lag 1 before shock)
  X_lagged <- lag.xts(
    covariates_series_list[[1]][1:integer_shock_time_vec[1], covariate_indices]
  )
  xreg_input <- X_lagged

  # For forecasting, repeat the last observed covariate vector 'k' times
  X_last <- covariates_series_list[[1]][integer_shock_time_vec[1], covariate_indices]
  xreg_future <- matrix(
    rep(X_last, forecast_horizon),
    nrow = forecast_horizon,
    byrow = TRUE
  )
}

# Auto‐detect seasonal period for the target if needed
target_seasonal_period <- seasonal_period
if (isTRUE(seasonal) && is.null(target_seasonal_period)) {
  target_seasonal_period <- tryCatch({
    forecast::findfrequency(as.numeric(target_series))
  }, error = function(e) {
    warning("Target: seasonal auto‐detect failed → default = 4")
    4
  })
}

# Fit the ARIMA model for the target series
if (!is.null(arima_order)) {
  if (isTRUE(seasonal) && !is.null(seasonal_order)) {
    # Include specified seasonal_order
    target_model <- forecast::Arima(
      y        = target_series,
      order    = arima_order,
      seasonal = list(order  = seasonal_order,
                      period = target_seasonal_period),
      xreg     = xreg_input
    )
  } else {
    # No seasonal component
    target_model <- forecast::Arima(
      y        = target_series,
      order    = arima_order,
      seasonal = FALSE,
      xreg     = xreg_input
    )
  }
} else {
  # Let auto.arima() choose the best model (possibly seasonal)
  target_model <- forecast::auto.arima(
    y        = target_series,
  xreg     = xreg_input,
  ic       = user_ic_choice,
  seasonal = seasonal,
  stepwise = stepwise,
  approximation = approximation
  )
}

# Generate k‐step ahead forecasts (with or without xreg)
if (is.null(xreg_future)) {
  unadjusted_pred <- predict(target_model, n.ahead = forecast_horizon)
} else {
  unadjusted_pred <- predict(
    target_model,
    n.ahead = forecast_horizon,
    newxreg = xreg_future
  )
}

# Finally, adjust the raw ARIMA forecast by adding the weighted shock effect
adjusted_pred <- unadjusted_pred$pred + omega_star_hat

# 1. take vectors from objects
  raw_vec       <- as.numeric(unadjusted_pred$pred)          # ARIMA-only
  adjusted_vec  <- as.numeric(adjusted_pred)                 # synthetic + shock
  arithmetic_vec<- raw_vec + mean(omega_star_hat_vec)        # arithmetic mean

  # 2. build predictionsn sublist
  predictions <- list(
    unadjusted      = raw_vec,
    adjusted        = adjusted_vec,
    arithmetic_mean = arithmetic_vec
  )

  # 3. optional loss sublist
  loss <- NULL
  if (exists("ground_truth_vec") && !is.null(ground_truth_vec)) {
    loss <- list(
      unadjusted      = sum((raw_vec        - ground_truth_vec)^2),
      adjusted        = sum((adjusted_vec   - ground_truth_vec)^2),
      arithmetic_mean = sum((arithmetic_vec - ground_truth_vec)^2)
    )
  }

  # 4. meta sublist
  meta <- list(
    n_donors       = n,
    shock_time     = shock_time_vec,
    shock_length   = shock_length_vec,
    dbw_status     = if (exists("dbw_output")) dbw_output[[2]] else NA,
    weights        = w_hat,
    omega_vec      = omega_star_hat_vec,
    combined_omega = as.numeric(omega_star_hat),
    arima_order    = order_of_arima,
    ic_used        = user_ic_choice
  )
  if (exists("omega_star_std_err_hat_vec")) {
    meta$omega_se <- omega_star_std_err_hat_vec
  }

  # 5. final output list
  output_list <- list(
    linear_combinations = w_hat,
    predictions         = predictions
  )
  if (!is.null(loss)) {
    output_list$loss <- loss
  }
  output_list$meta <- meta

  # 6. opitional
  if (plots) {
    plot_maker_synthprediction(
      Y_series_list,
      shock_time_vec,
      integer_shock_time_vec,
      shock_length_vec,
      raw_vec,
      w_hat,
      omega_star_hat_vec,
      adjusted_vec,
      arithmetic_vec,
      display_ground_truth = display_ground_truth_choice
    )
  }

  return(output_list)
}

```

## SynthPrediction Function — Key Enhancements

Compared to the original, single-path implementation, this version adds:

1. **User-specified ARIMA order**  
   - Introduces `arima_order`. When non-`NULL`, the function calls `forecast::Arima()` with the user’s `(p,d,q)` (and optionally `(P,D,Q)`), bypassing the default `auto.arima()` search.

2. **SARIMA (seasonal) support**  
   - Adds `seasonal`, `seasonal_order` and `seasonal_period` parameters.  
   - If `seasonal = TRUE` and no period is provided, automatically detects frequency via `forecast::findfrequency()`.  
   - Honors user-supplied seasonal orders when fitting with `Arima()`.

3. **Flexible auto-search controls**  
   - Exposes `stepwise` and `approximation` arguments in the `auto.arima()` call, letting users trade off speed vs. accuracy.  
   - Wraps the call in `tryCatch()` so that any fit failure falls back to a simple `Arima(1,1,1)` rather than crashing.

4. **Robust covariate lag & NA handling**  
   - After creating lagged regressors (`lag.xts()`), automatically drops leading `NA` rows via `na.omit()`, guaranteeing that the dependent series and `xreg` remain time-aligned.

5. **Character or numeric shock times**  
   - Accepts both date strings and index positions in `shock_time_vec`, converting either form into integer indices without manual user intervention.

6. **Fixed-effect extraction from donors**  
   - Uses `lmtest::coeftest()` on each donor’s fit to pull out the coefficient on the “post_shock_indicator” regressor, storing these in `omega_star_hat_vec`.

7. **Distance-based weighting integration**  
   - Calls the custom `dbw()` function to compute optimal donor weights `w_hat`, then forms the aggregate shock effect `omega_star_hat = w_hat %*% omega_star_hat_vec`.

8. **Unified multi-step forecasting & adjustment**  
   - Fits the target series with the same ARIMA/SARIMA logic.  
   - Generates an unadjusted forecast (via `predict()` or `forecast::forecast()`), then simply adds `omega_star_hat` to produce the “adjusted” forecast.

9. **Structured output and plotting**  
   - Returns a list containing both the donor weight vector and two forecast objects (`unadjusted_pred`, `adjusted_pred`).  
   - Includes a `plots` flag to trigger `plot_maker_synthprediction()`, producing side-by-side time-series comparisons.

10. **Verbose console feedback**  
    - Prints donor count, shock times, weights, MSE, and forecast comparisons via `cat()`, aiding interactive debugging and auditability.

## Next Steps

- **This week**: Test the function thoroughly using simulated data to ensure all branches work correctly.  
- **Next week**: If you have time, I’d like to schedule a meeting to review results and discuss the plan for further development.

# Testing with simulated data
## Overview

To ensure the correctness, robustness, and applicability of the `SynthPrediction()` function, we propose a 5-round testing strategy. The logic is progressive: each round builds on the last, covering increasing complexity and realistic challenges.

---

**Round 1: Basic Prediction without Covariates**

- **Goal**: Verify whether the function runs correctly without `covariates_series_list`.
- **Status**: Completed
- **Description**:
  - Confirm that the model can be trained and used to make predictions using only `Y_series_list`.
  - Ensure `dbw()` is bypassed and the function still returns valid forecasts.
 
**Round 2: Correctness of Covariate Mechanism**

- **Goal**: Ensure covariates are properly incorporated and improve prediction quality.
- **Status**: Completed
- **Description**:
  - Test that `covariates_series_list` is correctly passed into `dbw()`.
  - Check that enabling `covariate_indices` activates covariate weighting.
  - Evaluate whether predictions improve with informative covariates.

**Round 3: Covariate Sensitivity and Robustness Analysis**

- **Goal**: Evaluate the model’s behavior under different covariate configurations.
- **Status**: In progress
- **Description**:
  - Vary the number of covariates (e.g., 1, 3, 5).
  - Inject irrelevant or noisy covariates.
  - Introduce outliers into covariates.
  - Examine small-sample and high-dimensional edge cases.
  - Assess whether weight assignment and prediction accuracy are stable.

**Round 4: Shock Mechanism and Adjustment Logic Validation**

- **Goal**: Test the effectiveness of the prediction adjustment strategy under different types of shocks.
- **Status**: Not started
- **Description**:
  - Manually inject various types of shocks:
    - Sudden jumps (local shocks)
    - Level shifts (persistent shocks)
    - Volatility changes (variance shocks)
  - Check whether adjusted predictions provide meaningful improvements over unadjusted ones.

**Round 5: Realistic and Multi-Shock Simulation**

- **Goal**: Validate the framework under real-world-like conditions.
- **Status**: Not started
- **Description**:
  - Use complex or realistic synthetic datasets.
  - Simulate multiple shocks or prolonged events.
  - Evaluate the stability of `SynthPrediction()` in long-term, multi-event time series forecasting.

## Round 1: Basic Prediction without Covariates
```{r, message=FALSE, warning=FALSE,echo=FALSE}
library(forecast)   # Provides auto.arima, Arima, and forecast/predict functions
library(lmtest)     # Provides coeftest for extracting regression coefficients
library(xts)        # Provides the ’xts’ time-series data structure

set.seed(42)
# Generate a 200-day date index from 2021-01-01 to 2021-07-19
dates <- seq.Date(from = as.Date("2021-01-01"), by = "day", length.out = 200)

# Create the target series as a random walk with a constant offset of 100
y_target <- cumsum(rnorm(200, mean = 0, sd = 1)) + 100
Y0 <- xts(y_target, order.by = dates)   # Convert to an xts time series

# Create the donor series identical to the target (for structural testing)
Y1 <- Y0

# Combine target and donor into a list of time series
Y_series_list <- list(Y0, Y1)

# Build covariate series: here we supply a single “all-ones” covariate for each series
covariates_series_list <- list(
  xts(rep(1, 200), order.by = dates),  # Covariate for the target series
  xts(rep(1, 200), order.by = dates)   # Covariate for the donor series
)

# Define shock times and shock lengths: both series experience a “shock” at time index 150 for 1 step
shock_time_vec   <- c(150, 150)
shock_length_vec <- c(1, 1)

# Call the SynthPrediction function with minimal settings
res_minimal <- SynthPrediction(
  Y_series_list          = Y_series_list,          # List of target + donor series
  covariates_series_list = covariates_series_list, # List of covariate series (all-ones)
  shock_time_vec         = shock_time_vec,         # Indices at which shock begins for each series
  shock_length_vec       = shock_length_vec,       # Duration of shock for each series (1 step)
  k                      = 1,                      # Forecast horizon: predict 1 step ahead
  covariate_indices      = NULL,                   # Do not use any user-specified covariate columns
  seasonal               = FALSE,                  # Disable seasonal ARIMA components
  plots                  = FALSE                   # Disable plotting of results
)

# Inspect the structure of the returned object
str(res_minimal)

```
### 1. Key Modifications

1. **Skip `dbw()` if no covariates supplied**  
   - When `covariate_indices = NULL`, we do not call `dbw()`. Instead, we compute a simple equal‐weight average of donor shock coefficients (`w_hat = rep(1 / n_donors)`). This avoids passing an empty matrix into `dbw()` and prevents dimension errors.

2. **Use a single “dummy” covariate only for `dbw()` if needed**  
   - If real covariates exist, we call `dbw()` with the selected columns and a one‐step lag structure.  
   - If no real covariates are provided, we temporarily supply a list of “all‐ones” series—one for each target/donor—so that `dbw()` sees at least one column. Because all donors share the same constant covariate, `dbw()` effectively yields equal weights. After obtaining those weights, we drop the dummy covariate and use the resulting weighted shock effect.

3. **Refine ARIMA‐fitting logic for donors**  
   - **User‐specified order (`arima_order` not NULL)**: call `Arima()` with `(p, d, q)` and optional `(P, D, Q, period)` if seasonal components are desired.  
   - **Automatic selection (`arima_order = NULL`)**: wrap `auto.arima()` in `tryCatch()` so that, if ARIMA selection fails, we fall back to `Arima(1,1,1)`.  
   - In each donor model, we include the “post‐shock indicator” as an external regressor (xreg), then extract its coefficient via `lmtest::coeftest()`. That coefficient becomes that donor’s estimated fixed effect.

4. **Equal‐weight averaging when no covariates**  
   - If `covariate_indices = NULL`, we compute `w_hat` simply as a vector of length equal to the number of donors, each element = 1/number_of_donors.  
   - We then compute the aggregate shock effect `omega_star_hat` as the weighted sum of donor coefficients. This entirely replaces the `dbw()` step in the no‐covariate case.

5. **Preserve full `dbw()` pipeline when covariates are present**  
   - When the user does supply `covariate_indices`, we follow the original design: call `dbw(X = covariates_series_list, …, X_lookback_indices = rep(list(1), length(dbw_indices)), Y_lookback_indices = NULL, inputted_transformation = id)` to retrieve optimal weights.  
   - Then compute `omega_star_hat` by multiplying `w_hat` with the donor coefficients.

---

### 2. Testing Procedure

Below is a concise description of how we test under “minimal” conditions—no code is shown, only the logical steps:

1. **Simulate 200 time points with dates**  
   - Create a sequence of 200 daily dates.  
   - Generate a target series by simulating a random walk around a baseline (e.g., starting at 100).  
   - Create a single donor that is identical to the target (so that their shock coefficients match exactly).

2. **Construct “dummy” covariate series**  
   - Build one constant‐value series (all 1’s) for each time index and for each of target and donor. This ensures that, if we ever call `dbw()`, it sees at least one column of covariates.

3. **Define shock times and lengths**  
   - Both the target and the donor receive a “shock” at index t = 150, lasting exactly one step. We will forecast t = 151.

4. **Call `SynthPrediction()` with minimal settings**  
   - Set `covariate_indices = NULL` (no real covariates).  
   - Disable seasonal components (`seasonal = FALSE`).  
   - Request only a one‐step forecast (`k = 1`).  
   - Disable plotting (`plots = FALSE`) to focus on the returned data.

5. **Inspect the returned object**  
   - Confirm that `linear_combinations[[1]]` equals 1 (since only one donor → equal weight).  
   - Confirm that both `predictions$unadjusted_pred` and `predictions$adjusted_pred` each contain exactly one value, corresponding to t = 151.
   
### 3. Expected Outcome

- **Weight vector** `w_hat = 1` (only one donor, so equal weight = 1).  
- **Aggregate shock effect** `omega_star_hat` equals the donor’s estimated fixed effect.  
- **`unadjusted_pred`** is the plain ARIMA forecast for t = 151, which may be slightly off due to random noise.  
- **`adjusted_pred`** = `unadjusted_pred + omega_star_hat`, which should nearly equal the actual value at t = 151 (error ≈ 0).  
 
 
## Round 2: Correctness of Covariate Mechanism

In this round, we will test the `SynthPrediction()` function with a more complex setup that includes covariates. This will help us verify that the function correctly handles multiple donors, applies the distance‐based weighting, and integrates covariate information into the ARIMA modeling.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(123)  
# Fix the random seed so that generated random values are reproducible.
dates <- seq.Date(
  from = as.Date("2021-01-01"), 
  by   = "day", 
  length.out = 200
)
# Create a sequence of 200 daily dates starting from 2021-01-01;
# this will serve as the time index for our xts time series.
y0_values <- cumsum(rnorm(200, mean = 0, sd = 1)) + 100  
# Generate 200 independent N(0,1) random draws, then take their cumulative sum
# to simulate a random walk, and finally add 100 as a baseline offset.
Y0 <- xts(y0_values, order.by = dates)
# Convert y0_values into an xts time series, indexed by the "dates" sequence.
y1_values <- y0_values + rnorm(200, mean = 0, sd = 0.1)  
# Produce a “donor” series by taking Y0’s values and adding small N(0,0.1) noise,
# making Y1 very highly correlated with Y0 but not identical.
Y1 <- xts(y1_values, order.by = dates)
# Convert y1_values into an xts time series, sharing the same dates index.
y2_values <- y0_values + rnorm(200, mean = 0, sd = 2)  
# Produce another “donor” series by adding larger N(0,2) noise to Y0,
# so that Y2 is less correlated with Y0 than Y1 is.
Y2 <- xts(y2_values, order.by = dates)
# Convert y2_values into an xts time series, using the same dates index.
Y_series_list <- list(Y0, Y1, Y2)
# Bundle the three xts objects (target + two donors) into a list for input to SynthPrediction.
# 4. Construct corresponding covariate series X0, X1, X2 --------------------------
#    X0 and X1 are both built from the same underlying “base_signal”, so they will correlate well with Y0 and Y1.
base_signal <- cumsum(rnorm(200, mean = 0, sd = 1))  
# Generate a second random walk (“base signal”) by cumulatively summing 200 N(0,1) draws.
# This base_signal will be used to create correlated covariates.
X0_values  <- base_signal + rnorm(200, mean = 0, sd = 0.1)  
# Construct X0 by adding small N(0,0.1) noise to base_signal,
# ensuring X0 is highly correlated with base_signal (and thus with Y0/Y1 to some extent).
X1_values  <- base_signal + rnorm(200, mean = 0, sd = 0.1)  
# Construct X1 similarly, so that X1 is also highly correlated with base_signal (and thus with Y1).
#    X2 is generated independently, making it less informative about Y0/Y1/Y2.
X2_values  <- cumsum(rnorm(200, mean = 0, sd = 2))  
# Create a new random walk with larger noise (sd=2), so X2 is effectively uncorrelated with base_signal.
X0 <- xts(X0_values, order.by = dates)
# Convert X0_values to an xts object indexed by the same “dates”.
X1 <- xts(X1_values, order.by = dates)
# Convert X1_values to an xts object with identical dates index.
X2 <- xts(X2_values, order.by = dates)
# Convert X2_values to an xts object with the same dates index.
# 5. Define shock times and shock durations -------------------------------
#    We simulate an event (“shock”) occurring at time index 150 for all three series, lasting 1 step.
shock_time_vec   <- c(150, 150, 150)
# Each element corresponds to the time index where a shock begins:
# positions 1–3 match Y0, Y1, Y2 respectively.
shock_length_vec <- c(1, 1, 1)
# Each element indicates the shock’s length (in time steps),
# here set to 1 for all three series.
Y_series_list <- list(Y0, Y1, Y2)
covariates_series_list <- list(X0, X1, X2)

res_with_cov <- SynthPrediction(
  Y_series_list = Y_series_list,
  covariates_series_list = covariates_series_list,
  shock_time_vec = shock_time_vec,
  shock_length_vec = shock_length_vec,
  k = 1,
  covariate_indices = 1,   
  seasonal = FALSE,
  plots = FALSE
)
res_with_cov
Y_series_list[[1]][150:151]  
```

### Improvements to the `dbw()` Function

In this test, we enhanced the `dbw()` function to ensure robustness, especially in edge cases such as using single-column covariates or minimal lagged data. The key improvements are as follows:

#### 1. Preventing Dimension Reduction

All column extraction operations were updated to explicitly preserve matrix structure, preventing accidental conversion to vectors when extracting a single column. This ensures matrix operations such as binding, multiplication, and norm calculation function as intended.

---

#### 2. Input Dimensionality Check

A validation step was added to confirm that each covariate series contains enough columns to match the specified `dbw_indices`. If not, the function halts early with a clear error message, avoiding silent failures caused by empty or incomplete data extraction.

---

#### 3. Consistent Matrix Output in Lag Extraction

The internal lag extraction logic was revised to always return a matrix with consistent dimensions, even when extracting a single value. This guarantees that downstream steps such as row binding and singular value decomposition work reliably.

---

#### 4. Safeguard Against Empty Data Matrices

A check was added to detect cases where the extracted covariate matrix is empty (e.g., due to invalid lag settings or mismatched series). If triggered, the function provides immediate feedback and prevents execution on invalid data.

---

### 5. Cleaner Structure and Return Format

The function structure was streamlined by removing unnecessary intermediate variables and debugging output. The final output now directly returns a named list containing the optimized weights, convergence status, and approximation loss.

---

### Result

These modifications greatly improved the stability and reliability of the `dbw()` function, allowing it to handle minimal or irregular covariate input without failure. The enhanced version now produces consistent and interpretable results, even in low-dimensional or single-lag settings, making it suitable for synthetic control under shock events.

## Round 3: Covariate Sensitivity and Robustness Analysis 
- **Covariate setup**  
  - 1st covariate = `base_signal` + small noise (informative)  
  - Others = independent random walks (noise)

- **Tested k = 1, 3, 5**  
  - **k = 1**  
    - Weights ≈ (1.00, 0.00)  
    - RMSE = 0.072 
    - ⚠️ “Hessian inversion” warning → low-dim instability  
  - **k = 3**  
    - Weights ≈ (0.10405, 0.89595)  
    - RMSE = 0.693
  - **k = 5**  
    - Weights ≈ (0.68864, 0.31136)  
    - RMSE = 0.401

- **Key takeaway**  
  Higher proportion of noise covariates → worse RMSE.  
  Use small regularization (`penalty_lambda > 0`) for stable weight estimates in low-dim cases.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# --- Round 3: Covariate Sensitivity and Robustness Analysis ---
set.seed(123)
# 1. Prepare series length and date index
len   <- nrow(Y_series_list[[1]])
dates <- index(Y_series_list[[1]])

# 2. Covariate generator:
#    - 1st column carries base signal + small noise
#    - Remaining columns are independent random-walk noise
generate_covariates <- function(k) {
  base_signal <- cumsum(rnorm(len))
  mat <- sapply(seq_len(k), function(j) {
    if (j == 1) {
      base_signal + rnorm(len, 0, 0.1)
    } else {
      cumsum(rnorm(len, 0, 3))
    }
  })
  xts(mat, order.by = dates)
}

# 3. RMSE function
rmse <- function(pred, truth) {
  p <- coredata(pred)
  t <- coredata(truth)
  sqrt(mean((p - t)^2, na.rm = TRUE))
}

# 4. Test model performance for k = 1, 3, 5 covariates
results <- list()
for (k in c(1, 3, 5)) {
  cat("==== Now testing with", k, "covariates ====\n")
  
  # 4.1 Generate k covariates for each series
  cov_list <- lapply(seq_along(Y_series_list), function(i) 
    generate_covariates(k)
  )
  
  # 4.2 Run the SynthPrediction function
  res <- SynthPrediction(
    Y_series_list          = Y_series_list,
    covariates_series_list = cov_list,
    shock_time_vec         = shock_time_vec,
    shock_length_vec       = shock_length_vec,
    k                      = 1,
    covariate_indices      = 1:k,
    seasonal               = FALSE,
    plots                  = FALSE,
    penalty_lambda         = 1e-4,
    penalty_normchoice     = "l2"
  )
  
  # 4.3 Extract true and predicted values
  true_val  <- Y_series_list[[1]][ shock_time_vec[1] + 1 ]
  arima_val <- res$predictions$unadjusted
  synth_val <- res$predictions$adjusted
  mean_val  <- res$predictions$arithmetic_mean
  
  # 4.4 Compute RMSE for each method
  err_arima <- rmse(arima_val, true_val)
  err_synth <- rmse(synth_val, true_val)
  err_mean  <- rmse(mean_val,  true_val)
  
  # 4.5 Print comparison of true vs. predicted values and errors
  cat(sprintf(
    "k=%d | true=%.3f | ARIMA=%.3f(err=%.3f) | synth=%.3f(err=%.3f) | mean=%.3f(err=%.3f)\n",
    k,
    coredata(true_val),
    coredata(arima_val), err_arima,
    coredata(synth_val), err_synth,
    coredata(mean_val),  err_mean
  ))
  
  # 4.6 Save results into summary list
  results[[paste0("k_", k)]] <- list(
    rmse_arima    = err_arima,
    rmse_synth    = err_synth,
    rmse_mean_adj = err_mean,
    weights       = res$linear_combinations
  )
}

# 5. Inspect summarized results
str(results)


```

## Round 4 (fixed): Shock Mechanism & Adjustment Logic Validation with Proper Alignment

```{r Round4_fixed, message=FALSE, warning=FALSE, echo=FALSE}
library(xts)
library(forecast)
library(lmtest)
library(TTR)

# —— 1. Define a function to compute cleaned technical indicators —— #
make_technical_indicators <- function(Y) {
  # 1a) Compute raw indicators (may include NA values at the beginning)
  raw_df <- cbind(
    MA5  = runMean(coredata(Y), n = 5),      # 5-period moving average
    MA10 = runMean(coredata(Y), n = 10),     # 10-period moving average
    Vol  = runSD(coredata(Y),   n = 10),     # 10-period rolling standard deviation (volatility)
    Mom  = c(NA, diff(coredata(Y)))          # true momentum: Y[t] - Y[t-1]
  )
  
  # 1b) Identify rows with no missing values
  good <- complete.cases(raw_df)
  
  # 1c) Subset the raw matrix and align timestamps
  df  <- raw_df[good, , drop = FALSE]
  idx <- index(Y)[good]
  
  # 1d) Return as an xts object with the filtered time index
  xts(df, order.by = idx)
}

# —— 2. Generate cleaned covariate list for each series Y0, Y1, Y2 —— #
cov_list_clean <- lapply(list(Y0, Y1, Y2), make_technical_indicators)

# —— 3. Align the original series to the covariate timestamps —— #
common_index <- index(cov_list_clean[[1]])
Y_clean_list <- lapply(list(Y0, Y1, Y2), `[`, common_index)

# —— 4. Print and verify the covariates and aligned series —— #
cat("== First 10 rows of covariates (MA5, MA10, Vol, Mom) ==\n")
print(head(cov_list_clean[[1]], 10))

cat("\n== Summary of covariates ==\n")
print(summary(cov_list_clean[[1]]))

cat("\n== First 10 rows of aligned target series Y0 ==\n")
print(head(Y_clean_list[[1]], 10))

# —— 5. Define the shock parameters —— #
t0_index   <- 150
shock_date <- as.character(common_index[t0_index])
cat("\nShock date identified as:", shock_date, "\n")

# —— 6. Run the synthetic prediction with shock adjustment —— #
res_round4 <- SynthPrediction(
  Y_series_list          = Y_clean_list,
  covariates_series_list = cov_list_clean,
  shock_time_vec         = shock_time_vec,
  shock_length_vec       = shock_length_vec,
  k                      = 1,
  covariate_indices      = 1:ncol(cov_list_clean[[1]]),
  seasonal               = FALSE,
  plots                  = FALSE,      # disable built-in plotting
  penalty_lambda         = 1e-4,
  penalty_normchoice     = "l2"
)

print(res_round4$predictions$unadjusted)
print(res_round4$predictions$adjusted)
print(res_round4$predictions$arithmetic_mean)

```

```{r Round4_results, message=FALSE, warning=FALSE}
library(xts)

# —— 1. Setup: define shock time index & date —— #
# Assume Y_clean_list is already available: [[1]] is the treated series,
# [[2]]~[[n]] are the donor series, and there are at least 155 observations.
t0_index <- 150
t0_date  <- index(Y_clean_list[[1]])[t0_index]

# —— 2. Build three shock scenarios —— #
# 2.1 Local Jump: at t0+1 jump up by +20, then revert to original path
Y_jump <- lapply(Y_clean_list, function(x) {
  x2 <- x
  x2[t0_index + 1] <- coredata(x2[t0_index - 1]) + 20
  x2
})

# 2.2 Level Shift: from t0+1 onward, add a permanent +5 elevation
Y_level <- lapply(Y_clean_list, function(x) {
  x2 <- x
  idx <- (t0_index + 1):nrow(x2)
  x2[idx] <- coredata(x2[idx]) + 5
  x2
})

# 2.3 Volatility Shock: from t0+1 onward, add N(0,3) noise each period
set.seed(42)
Y_vol <- lapply(Y_clean_list, function(x) {
  x2 <- x
  idx <- (t0_index + 1):nrow(x2)
  x2[idx] <- coredata(x2[idx]) + rnorm(length(idx), mean = 0, sd = 3)
  x2
})

# —— 3. Combine into a named list of scenarios —— #
scenarios <- list(
  Jump  = Y_jump,
  Level = Y_level,
  Vol   = Y_vol
)

# —— 4. For each scenario, print the ±5 day window around the shock —— #
window_half <- 5
for (name in names(scenarios)) {
  # extract the treated series for this scenario
  Y_scn <- scenarios[[name]][[1]]
  
  # compute the index range, handling boundaries
  start_idx <- max(1,                 t0_index - window_half)
  end_idx   <- min(nrow(Y_scn),       t0_index + window_half)
  idx_range <- start_idx:end_idx
  
  cat(sprintf(
    "\n=== %s Shock: t0 = %s (index %d), ±%d days ===\n",
    name, as.character(t0_date), t0_index, window_half
  ))
  
  # print the subset of the series
  print(Y_scn[idx_range, , drop = FALSE])
}


```

```{r Round4_test, message=FALSE, warning=FALSE, echo=FALSE}
# — Basic setup —
n_series       <- length(Y_clean_list)
t0_index       <- 150
shock_time_vec <- rep(t0_index, n_series)
len_post       <- nrow(Y_clean_list[[1]]) - t0_index

# shock lengths for each scenario
shock_length_jump  <- rep(1,       n_series)  # only next period
shock_length_level <- rep(len_post, n_series) # from t0+1 to end
shock_length_vol   <- rep(len_post, n_series) # idem

# covariates and indices
covars  <- cov_list_clean
cov_idx <- seq_len(ncol(covars[[1]]))

# public params
k_steps  <- 1
seasonal <- FALSE
plots    <- FALSE
λ        <- 1e-4
norm     <- "l2"

# — 1. Local Jump (+20 at t0) —
true_jump <- as.numeric(Y_jump[[1]][t0_index + 1])
res_jump  <- SynthPrediction(
  Y_series_list          = Y_jump,
  covariates_series_list = covars,
  shock_time_vec         = shock_time_vec,
  shock_length_vec       = shock_length_jump,
  k                      = k_steps,
  covariate_indices      = cov_idx,
  seasonal               = seasonal,
  plots                  = plots,
  penalty_lambda         = λ,
  penalty_normchoice     = norm
)
raw_jump <- as.numeric(res_jump$predictions$unadjusted)
syn_jump <- as.numeric(res_jump$predictions$adjusted)

cat(sprintf(
  "Jump  | true=%.3f | ARIMA=%.3f (err=%.3f) | synth+shock=%.3f (err=%.3f)\n\n",
  true_jump,
  raw_jump, abs(raw_jump - true_jump),
  syn_jump, abs(syn_jump - true_jump)
))

# (Optional) dump full structure for Jump
cat("---- Full res_jump object ----\n")
str(res_jump)
cat("---- End of res_jump dump ----\n\n")


# — 2. Level Shift (+5 from t0+1 onward) —
true_level <- as.numeric(Y_level[[1]][t0_index + 1])
res_level  <- SynthPrediction(
  Y_series_list          = Y_level,
  covariates_series_list = covars,
  shock_time_vec         = shock_time_vec,
  shock_length_vec       = shock_length_level,
  k                      = k_steps,
  covariate_indices      = cov_idx,
  seasonal               = seasonal,
  plots                  = plots,
  penalty_lambda         = λ,
  penalty_normchoice     = norm
)
raw_level <- as.numeric(res_level$predictions$unadjusted)
syn_level <- as.numeric(res_level$predictions$adjusted)

cat(sprintf(
  "Level | true=%.3f | ARIMA=%.3f (err=%.3f) | synth+shock=%.3f (err=%.3f)\n\n",
  true_level,
  raw_level, abs(raw_level - true_level),
  syn_level, abs(syn_level - true_level)
))


# — 3. Volatility Shock (iid N(0,σ) noise from t0+1) —
true_vol <- as.numeric(Y_vol[[1]][t0_index + 1])
res_vol  <- SynthPrediction(
  Y_series_list          = Y_vol,
  covariates_series_list = covars,
  shock_time_vec         = shock_time_vec,
  shock_length_vec       = shock_length_vol,
  k                      = k_steps,
  covariate_indices      = cov_idx,
  seasonal               = seasonal,
  plots                  = plots,
  penalty_lambda         = λ,
  penalty_normchoice     = norm
)
raw_vol <- as.numeric(res_vol$predictions$unadjusted)
syn_vol <- as.numeric(res_vol$predictions$adjusted)

cat(sprintf(
  "Vol   | true=%.3f | ARIMA=%.3f (err=%.3f) | synth+shock=%.3f (err=%.3f)\n",
  true_vol,
  raw_vol, abs(raw_vol - true_vol),
  syn_vol, abs(syn_vol - true_vol)
))

```
### Round 4 Summary
 
Local Jump (+20 at t₀):

ARIMA misses the spike (err ≈ 19.1)

Synth+Shock captures it (err ≈ 0.5)

Level Shift (+5 from t₀+1 onward):

ARIMA lags behind (err ≈ 4.7)

Synth+Shock adjusts almost exactly (err ≈ 0.35)

Volatility Shock (iid N(0,3) noise from t₀+1):

Both methods perform similarly (err ≈ 3.6–3.8)

Random noise is hard to anticipate in a single step

Key points:

Synth+Shock excels at systematic breaks (jumps or shifts).

For pure noise shocks, one‐step adjustment yields only a small gain.

Over longer horizons (k > 1), the adjustment accumulates (≈ ω⋆ × k), so persistent shocks will stand out more.

This template—(1) generate scenarios → (2) verify ±5 days around t₀ → (3) run SynthPrediction → (4) tabulate errors—can be reused for any new shock‐forecasting experiment.