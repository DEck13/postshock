---
title: "GARCH_part"
author: "Qiyang Wang"
date: "`r Sys.Date()`"
output: pdf_document
---
## import the dbw and QL loss function
```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START QL_loss_function
QL_loss_function <- function(pred, gt){gt/pred - log(gt/pred) - 1}
### END QL_loss_function

#We specify some functions for transforming y

#mean_square_y will be used in garch models
mean_square_y <- function(y){return((y-mean(y))**2)}

#identity function will be used in synthetic prediction models
id <- function(y){return(y)}

```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
### START dbw
dbw <- function(
  X,                        # List of covariate time series (X[[1]] = target, X[[2]]~X[[n+1]] = donors)
  dbw_indices,              # Indices of covariate columns to use (e.g., 1, 1:3, etc.)
  shock_time_vec,           # Vector of shock time indices (one per series)
  scale = FALSE,            # Whether to standardize covariate columns (scale to unit variance)
  center = FALSE,           # Whether to mean-center covariate columns
  sum_to_1 = 1,             # Whether weights must sum to 1 (set to 1 for simplex constraints)
  bounded_below_by = 0,     # Lower bound for weights (e.g., 0 for non-negativity)
  bounded_above_by = 1,     # Upper bound for weights (e.g., 1 for simplex)
  princ_comp_count = NULL,  # Number of principal components to use (NULL = no PCA)
  normchoice = 'l2',        # Distance metric to minimize ('l1' = Manhattan, 'l2' = Euclidean)
  penalty_normchoice = 'l1',# Norm for regularization penalty ('l1' = LASSO, 'l2' = Ridge)
  penalty_lambda = 0,       # Regularization strength (λ)
  Y = NULL,                 # Optional: list of Y series for transformation-based augmentation
  Y_lookback_indices = NULL,# Lags to use for Y if given (rarely used)
  X_lookback_indices = NULL,# Lags to use for each X covariate (defaults to last row)
  inputted_transformation   # Transformation function to apply to Y (e.g., identity or mean_square_y)
) {
  n <- length(X) - 1             # Number of donors
  p <- length(dbw_indices)       # Number of covariates used
  normchoice_number <- unlist(strsplit(normchoice, split = ""))[2]  # '1' or '2'

  # === 1. Safety check: Make sure all X[[i]] have enough columns ===
  for (i in seq_along(X)) {
    if (ncol(X[[i]]) < max(dbw_indices)) {
      stop(paste0("X[[", i, "]] has only ", ncol(X[[i]]),
                  " column(s), but dbw_indices = ", paste(dbw_indices, collapse = ",")))
    }
  }

  # === 2. Extract selected covariate columns from each series ===
  col_returner <- function(df) {
    return(df[, dbw_indices, drop = FALSE])
  }
  X_subset1 <- lapply(X, col_returner)

  # === 3. If Y is provided and lookback is specified, merge Y transformation with X (rarely used) ===
  if (!is.null(Y_lookback_indices) && !is.null(Y)) {
    X_lookback_indices <- c(Y_lookback_indices, X_lookback_indices)
    X_Y_combiner <- function(y, x) {
      transformed_y <- inputted_transformation(y)
      return(merge(transformed_y, x, all = FALSE))
    }
    combined_X <- mapply(X_Y_combiner, y = Y, x = X_subset1, SIMPLIFY = FALSE)
  } else {
    combined_X <- X_subset1
  }

  # === 4. Trim each series up to the shock time for that unit ===
  row_returner <- function(df, stv) {
    return(df[1:stv, , drop = FALSE])
  }
  X_subset2 <- mapply(row_returner, df = combined_X, stv = shock_time_vec, SIMPLIFY = FALSE)

  # === 5. Extract specified lagged rows for each covariate (default: last row before shock) ===
  cov_extractor <- function(X_df) {
    if (is.null(X_lookback_indices)) {
      # By default, use last row (pre-shock covariate value)
      vals <- as.numeric(tail(X_df, 1))
      return(matrix(vals, nrow = 1))
    } else {
      len <- nrow(X_df)
      padded_vector_maker <- function(x) {
        vec <- rep(FALSE, len)
        vec[x] <- TRUE
        rev(vec)
      }
      boolmat <- as.matrix(do.call(data.frame, lapply(X_lookback_indices, padded_vector_maker)))
      vals <- as.matrix(X_df)[boolmat]
      return(matrix(vals, nrow = 1))
    }
  }
  X_subset <- lapply(X_subset2, cov_extractor)

  # === 6. Combine all extracted rows into a data matrix (target in first row, donors below) ===
  dat <- do.call('rbind', X_subset)

  # === 7. Check for invalid data matrix ===
  if (nrow(dat) == 0 || ncol(dat) == 0) {
    stop("Covariate matrix 'dat' is empty. Check dbw_indices or lag settings.")
  }

  # === 8. Automatically remove constant covariates (zero standard deviation) ===
  const_cols <- which(apply(dat, 2, function(x) sd(x, na.rm = TRUE) == 0))
  if (length(const_cols) > 0) {
    warning(sprintf("⚠️ [DBW] Covariate column(s) %s are constant, automatically removed.", paste(const_cols, collapse=",")))
    dat <- dat[, -const_cols, drop = FALSE]
  }

  # === 9. Optionally standardize/center covariates ===
  if (scale || center) {
    dat <- scale(dat, center = center, scale = scale)
    dat <- as.matrix(dat)
  }

  # === 10. Check for NA or Inf in covariate matrix ===
  if (any(is.na(dat)) || any(is.infinite(dat))) {
    print(dat)
    stop("❌ [DBW] Covariate matrix dat contains NA or Inf! Check your input data.")
  }

  # === 11. Warn for remaining constant columns or severe collinearity ===
  const_cols <- which(apply(dat, 2, function(x) sd(x, na.rm = TRUE) == 0))
  if (length(const_cols) > 0) {
    warning(sprintf("⚠️ [DBW] Covariate column(s) %s are constant. Consider removing unnecessary covariates.", paste(const_cols, collapse=",")))
  }
  if (qr(dat)$rank < min(dim(dat))) {
    warning("⚠️ [DBW] Covariate matrix dat is rank-deficient (collinear). Check for perfectly correlated or duplicated variables!")
  }

  # === 12. Optional PCA reduction ===
  if (!is.null(princ_comp_count)) {
    if (ncol(dat) < princ_comp_count) stop("princ_comp_count > number of covariates")
    if (ncol(dat) == 1) stop("princ_comp_count should not be set for dat with only one column")
    dat.svd <- svd(dat)
    dat <- dat %*% dat.svd$v[, 1:princ_comp_count, drop = FALSE]
  }

  # === 13. Split matrix into target (X1) and donor covariates (X0) ===
  X1 <- dat[1, , drop = FALSE]                 # Target row
  dat_donors <- dat[-1, , drop = FALSE]        # Donor rows
  X0 <- split(dat_donors, seq(nrow(dat_donors))) # List of donor rows

  # === 14. Define the loss/objective function for DBW optimization ===
  weightedX0 <- function(W) {
    # Compute weighted donor covariate mean
    XW <- Reduce(`+`, Map(`*`, W, X0))
    # Distance between target and synthetic control (normchoice: l1 or l2)
    loss <- as.numeric(norm(X1 - XW, type = normchoice_number))
    # Add regularization penalty if penalty_lambda > 0
    if (penalty_lambda > 0) {
      if (penalty_normchoice == 'l1') {
        loss <- loss + penalty_lambda * sum(abs(W))
      } else if (penalty_normchoice == 'l2') {
        loss <- loss + penalty_lambda * sqrt(sum(W^2))
      }
    }
    return(loss)
  }

  # === 15. Set up optimization constraints (sum-to-1, bounds) ===
  eq_constraint <- if (!is.na(sum_to_1)) function(W) sum(W) - 1 else NULL
  lower_bound <- if (!is.na(bounded_below_by)) rep(bounded_below_by, n) else NULL
  upper_bound <- if (!is.na(bounded_above_by)) rep(bounded_above_by, n) else NULL

  # === 16. Run optimization using Rsolnp ===
  object_to_return <- Rsolnp::solnp(
    par = rep(1/n, n),          # Initial guess: uniform weights
    fun = weightedX0,           # Objective function to minimize
    eqfun = eq_constraint,      # Equality constraint (if any)
    eqB = 0,
    LB = lower_bound,
    UB = upper_bound,
    control = list(trace = 1, tol = 1e-8)
  )

  # === 17. Compute approximation loss (optional) ===
  loss <- round(norm(X1 - object_to_return$pars %*% dat_donors, type = normchoice_number), 3)
  convergence <- if (object_to_return$convergence == 0) "convergence" else "failed_convergence"

  # === 18. Return optimal weights and diagnostic status ===
  return(list(
    opt_params = object_to_return$pars,    # Optimal donor weights
    convergence = convergence,             # Convergence status (success/failure)
    loss = loss                            # Final distance between X1 and synthetic X0
  ))
}
### END dbw

```

## Auto.garch
```{r, message=FALSE, warning=FALSE,echo=FALSE}
auto_garchx <- function(y,
                        xreg         = NULL,
                        max.p         = 3,
                        max.q         = 3,
                        final_refit   = TRUE) {
  # y           : numeric vector of returns/residuals
  # xreg        : optional n×k matrix of exogenous regressors
  # max.p, max.q: search range for ARCH(p) and GARCH(q)
  # final_refit : if TRUE, do one extra backcast‐refit
  
  stopifnot(is.numeric(y), is.vector(y))
  n <- length(y)
  if (!is.null(xreg)) {
    xreg <- as.matrix(xreg)
    stopifnot(nrow(xreg) == n)
  }

  best <- list(bic = Inf, fit = NULL, p = NA_integer_, q = NA_integer_)

  # 1) Grid‐search over (p,1,q)
  for (p in seq_len(max.p)) {
    for (q in seq_len(max.q)) {
      fit0 <- try(
        garchx::garchx(
          y     = y,
          order = c(p, 1, q),
          xreg  = xreg
        ),
        silent = TRUE
      )
      if (inherits(fit0, "try-error")) next

      # 手动算 BIC = -2 logLik + (#coef)*log(n)
      ll   <- as.numeric(logLik(fit0))
      k    <- length(coef(fit0))
      bic0 <- -2 * ll + k * log(n)

      if (bic0 < best$bic) {
        best <- list(bic = bic0, fit = fit0, p = p, q = q)
      }
    }
  }

  if (is.null(best$fit)) {
    stop("auto_garchx: no converged model in grid search")
  }
  if (!final_refit) {
    return(best)
  }

  # 2) Compute theoretical unconditional variance
  cf          <- coef(best$fit)
  omega_hat   <- if("intercept" %in% names(cf)) cf["intercept"] else cf["omega"]
  alpha_hat   <- sum(cf[grep("^arch", names(cf), ignore.case=TRUE)])
  beta_hat    <- sum(cf[grep("^garch", names(cf), ignore.case=TRUE)])
  uncond_var  <- omega_hat / (1 - alpha_hat - beta_hat)

  message(sprintf("[auto_garchx] final refit with backcast = %.4f", uncond_var))

  # 3) 第二次 refit，一定要传 list(sigma2=...)
  fit2 <- try(
    garchx::garchx(
      y               = y,
      order           = c(best$p, 1, best$q),
      xreg            = xreg,
      backcast.values = list(sigma2 = uncond_var)
    ),
    silent = TRUE
  )
  if (inherits(fit2, "try-error")) {
    warning("[auto_garchx] backcast-refit failed, returning grid-search model")
    return(best)
  }
  best$fit <- fit2
  best$bic <- {
    ll2 <- as.numeric(logLik(fit2))
    k2  <- length(coef(fit2))
    -2*ll2 + k2*log(n)
  }
  best
}

```


## GARCH_part
```{r, message=FALSE, warning=FALSE,echo=FALSE}
SynthVolForecast <- function(
  Y_series_list,              # list: 1st is target, rest are donors
  covariates_series_list,     # list: donor covariates (length = n_donors)
  shock_time_vec,             # length = length(Y_series_list)
  shock_length_vec,           # length = length(Y_series_list)
  k = 1,
  dbw_scale = TRUE,
  dbw_center = TRUE,
  dbw_indices = NULL,
  princ_comp_input = NULL,
  covariate_indices = NULL,   # If using covariates for GARCH-X
  penalty_lambda = 0,
  penalty_normchoice = "l1",
  plots = TRUE,
  ground_truth_vec = NULL,
  shock_time_labels = NULL,
  return_fits = FALSE,
  garch_order      = NULL,
  max.p            = 3,
  max.q            = 3,
  double.backcast  = FALSE,
  backcast.initial = NULL
) {
  set.seed(123)
  n_total <- length(Y_series_list)
  n_donors <- n_total - 1

  # --- Check for length consistency ---
  stopifnot(length(shock_time_vec) == n_total)
  stopifnot(length(shock_length_vec) == n_total)
  stopifnot(length(covariates_series_list) == n_donors)
  
  # --- Standardize covariate structure: force data.frame, one per donor ---
  for (i in seq_len(n_donors)) {
    x <- covariates_series_list[[i]]
    # Allow numeric vector, data.frame, or matrix
    if (is.numeric(x)) {
      covariates_series_list[[i]] <- as.data.frame(x)
      colnames(covariates_series_list[[i]]) <- paste0("V", seq_along(x))
    } else if (is.matrix(x)) {
      covariates_series_list[[i]] <- as.data.frame(x)
    } else if (!inherits(x, "data.frame")) {
      stop(sprintf("covariates_series_list[[%d]] must be numeric, matrix, or data.frame", i))
    }
  }
  # Supply default indices for DBW if not specified
  if (is.null(dbw_indices)) {
    dbw_indices <- 1:ncol(covariates_series_list[[1]])
  }
  # Principal component input: if not specified, use the min of (number of series, covariates)
  if (is.null(princ_comp_input)) {
    princ_comp_input <- min(length(shock_time_vec), ncol(covariates_series_list[[1]]))
  }

  # --- Fit GARCH-X for each donor individually ---
  omega_star_hat_vec     <- numeric(n_donors)
  omega_star_std_err_vec <- numeric(n_donors)
  donor_fits             <- vector("list", n_donors)
  donor_selected_orders  <- vector("list", n_donors)

  for (i in seq_len(n_donors)) {
    donor_Y <- Y_series_list[[i+1]]
    donor_X <- covariates_series_list[[i]]

    # Construct post-shock indicator
    len_i   <- shock_length_vec[i+1]
    start_i <- shock_time_vec[i+1]
    end_i   <- min(start_i + len_i, length(donor_Y))
    post_shock_indicator <- rep(0, length(donor_Y))
    post_shock_indicator[(start_i+1):end_i] <- 1

    # Build final X for this donor
    rows_to_use <- length(donor_Y)
    if (is.null(covariate_indices)) {
      X_i_final <- matrix(post_shock_indicator[1:rows_to_use], ncol = 1,
                          dimnames = list(NULL, "post_shock_indicator"))
    } else {
      X_cov      <- as.matrix(donor_X[1:rows_to_use, covariate_indices, drop = FALSE])
      X_i_final  <- cbind(X_cov, post_shock_indicator[1:rows_to_use])
      colnames(X_i_final)[ncol(X_i_final)] <- "post_shock_indicator"
    }

    cat(sprintf("Donor %d: Y rows=%d, X rows=%d\n",
                i, length(donor_Y), nrow(X_i_final)))

    # --- Automatic/manual GARCH order selection and fit ---
    if (is.null(garch_order)) {
      sel <- auto_garchx(
        y               = donor_Y[1:rows_to_use],
        xreg            = X_i_final,
        max.p           = max.p,
        max.q           = max.q,
        backcast        = backcast.initial,
        double.backcast = double.backcast
      )
      order_i   <- c(sel$p, 1, sel$q)
      donor_fit <- sel$fit
      cat(sprintf("  auto‐selected order=(%d,1,%d)  BIC=%.2f\n",
                  sel$p, sel$q, sel$bic))
    } else {
      order_i   <- garch_order
      donor_fit <- garchx::garchx(
        y               = donor_Y[1:rows_to_use],
        order           = order_i,
        xreg            = X_i_final,
        backcast.values = backcast.initial
      )
      cat(sprintf("  using fixed order=(%d,%d,%d)\n",
                  order_i[1], order_i[2], order_i[3]))
    }

    # Save results
    donor_selected_orders[[i]] <- order_i
    donor_fits[[i]]            <- donor_fit

    # Extract post-shock effect (omega_star)
    ct <- lmtest::coeftest(donor_fit)
    omega_star_hat_vec[i]     <- ct["post_shock_indicator","Estimate"]
    omega_star_std_err_vec[i] <- ct["post_shock_indicator","Std. Error"]
  }

  # --- Target series: forecast before shock ---
  target_Y   <- Y_series_list[[1]]
  target_end <- shock_time_vec[1]

  # === DBW weighting ===
  n_donors <- length(covariates_series_list)
  dbw_output <- dbw(
    X = covariates_series_list,               # Only donors' covariate series (exclude target, no extra lag param)
    dbw_indices = dbw_indices,
    shock_time_vec = shock_time_vec[-1],      # Only donors' shock times
    scale = dbw_scale,
    center = dbw_center,
    sum_to_1 = TRUE,
    bounded_below_by = 0,
    bounded_above_by = 1,
    inputted_transformation = mean_square_y   # Transformation as per your paper
    # Other arguments left at default
  )
  w_hat <- dbw_output$opt_params
  omega_star_hat <- sum(w_hat * omega_star_hat_vec)

  # --- Target GARCH-X model (pre-shock) ---
  if (is.null(covariate_indices)) {
    target_xreg <- NULL
  } else {
    target_X    <- covariates_series_list[[1]]
    target_xreg <- as.matrix(
      target_X[1:target_end, covariate_indices, drop = FALSE]
    )
  }

  # 2) Auto/fixed order selection
  if (is.null(garch_order)) {
    sel_t <- auto_garchx(
      y               = target_Y[1:target_end],
      xreg            = target_xreg,
      max.p           = max.p,
      max.q           = max.q,
      backcast        = backcast.initial,
      double.backcast = double.backcast
    )
    target_order <- c(sel_t$p, 1, sel_t$q)
    fit_target   <- sel_t$fit
    message(sprintf(
      "[Target] auto‐selected order = (%d,1,%d), BIC=%.2f",
      sel_t$p, sel_t$q, sel_t$bic
    ))
  } else {
    target_order <- garch_order
    fit_target   <- garchx::garchx(
      y               = target_Y[1:target_end],
      order           = target_order,
      xreg            = target_xreg,
      backcast.values = backcast.initial
    )
    message(sprintf(
      "[Target] using fixed order = (%d,%d,%d)",
      target_order[1], target_order[2], target_order[3]
    ))
  }

  # 3) Forecast (after-shock horizon)
  raw_pred <- predict(fit_target, n.ahead = k)
  raw_vec  <- as.numeric(if (is.list(raw_pred)) raw_pred$pred else raw_pred)

  # 4) Adjusted forecasts (synthetic effect added)
  adjusted_vec   <- raw_vec + omega_star_hat
  arithmetic_vec <- raw_vec + mean(omega_star_hat_vec)

  predictions <- list(
    unadjusted      = raw_vec,
    adjusted        = adjusted_vec,
    arithmetic_mean = arithmetic_vec
  )

  # 5) If ground truth exists, calculate loss
  loss <- if (!is.null(ground_truth_vec)) {
    list(
      unadjusted      = sum(QL_loss_function(raw_vec, ground_truth_vec)),
      adjusted        = sum(QL_loss_function(adjusted_vec, ground_truth_vec)),
      arithmetic_mean = sum(QL_loss_function(arithmetic_vec, ground_truth_vec))
    )
  } else NULL

  # 6) Meta information
  meta <- list(
    n_donors       = n_donors,
    shock_time     = shock_time_vec,
    shock_length   = shock_length_vec,
    dbw_status     = dbw_output$convergence,
    weights        = w_hat,
    omega_vec      = omega_star_hat_vec,
    omega_se       = omega_star_std_err_vec,
    combined_omega = omega_star_hat,
    donor_orders   = donor_selected_orders,
    target_order   = target_order
  )

  # === Construct output: main components ===
  output_list <- list(
    linear_combinations = w_hat,         # DBW weights for linear combination
    predictions        = predictions,    # All forecast types
    meta               = meta            # Meta info
  )
  if (!is.null(loss)) output_list$loss <- loss

  # Optionally include full fit objects for research/debug
  if (return_fits) {
    output_list$donor_fits <- donor_fits   # All donor GARCH fits
    output_list$target_fit <- fit_target   # Target GARCH fit
  }

  # === Optional plotting (uses external function) ===
  if (plots) {
    plot_maker_garch(
      fitted(fit_target),
      shock_time_labels,
      shock_time_vec,
      shock_length_vec,
      raw_vec,
      w_hat,
      omega_star_hat_vec,
      omega_star_std_err_vec,
      adjusted_vec,
      arithmetic_vec,
      ground_truth_vec
    )
  }

  # === Return the final output ===
  return(output_list)
}


```

## Test the SynthVolForecast function
### First round of testing-for each module
#### Test Auto.garchx
```{r, message=FALSE, warning=FALSE,echo=FALSE}
library(garchx)
set.seed(123)

# 1. Simulate data: simple standard normal shocks x_t
n          <- 600
omega      <- 0.1
alpha      <- 0.15
beta       <- 0.8
gamma_coef <- 0.2

x <- rnorm(n)                # Direct standard normal white noise
x_mat <- matrix(gamma_coef * x, ncol = 1)
sim_all <- garchxSim(
  n         = n,
  intercept = omega,
  arch      = alpha,
  garch     = beta,
  xreg      = x_mat,      # Exogenous regressor, not used by sim_all directly
  verbose   = TRUE,
  as.zoo    = FALSE
)
# sim_all contains: y, sigma, sigma2, Ineg, innovations
y_all      <- sim_all[,"y"]
sigma2_all <- sim_all[,"sigma2"]
x_all      <- x_mat      # Use the user-constructed matrix directly

# Fit auto_garchx model to the data
res <- auto_garchx(
  y           = y_all,
  xreg        = x,
  max.p       = 3,
  max.q       = 3,
  final_refit = TRUE
)

# Obtain the result from auto_garchx
res <- auto_garchx(y = y_all, xreg = x, max.p = 3, max.q = 3, final_refit = TRUE)

# Check the selected lag orders and BIC
res$p        # Selected p (ARCH order)
res$q        # Selected q (GARCH order)
res$bic      # BIC value for the selected model

# Check if the returned fit object is the refitted model with backcast
res$fit

```
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# —— 1) Simulate pure GARCH(1,1) data —— 
# —— 1) Simulate pure GARCH(1,1) data —— 
set.seed(2025)
n_total <- 600
omega   <- 0.1
alpha   <- 0.15
beta    <- 0.8
sim_all <- garchxSim(
  n         = n_total,
  intercept = omega,
  arch      = alpha,
  garch     = beta,
  # by omitting xreg, we simulate a standard GARCH model
  verbose   = TRUE,
  as.zoo    = FALSE
)
y_all        <- sim_all[,"y"]
variance_all <- sim_all[,"sigma2"]

# —— 2) Prepare rolling forecast parameters —— 
train_len <- 500
horizon   <- n_total - train_len  # number of forecast steps (100)
pred_var  <- numeric(horizon)
true_var  <- variance_all[(train_len+1):n_total]

# —— 3) Rolling 1-step forecasts —— 
for (i in seq_len(horizon)) {
  # define the training series up to the time before the forecast
  y_tr <- y_all[1:(train_len + i - 1)]
  
  # fit a GARCH model automatically (no exogenous variables)
  res <- auto_garchx(
    y           = y_tr,
    xreg        = NULL,    # key: exclude any exogenous regressors
    max.p       = 3,
    max.q       = 3,
    final_refit = TRUE     # perform final backcast‐based refit
  )
  
  # one‐step ahead forecast of the conditional sd, then square to get variance
  fc           <- predict(res$fit, n.ahead = 1)
  pred_var[i]  <- as.numeric(fc)^2
}

# —— 4) Calculate accuracy metrics & plot results —— 
MAE  <- mean(abs(pred_var - true_var))
RMSE <- sqrt(mean((pred_var - true_var)^2))
MAPE <- mean(abs(pred_var - true_var) / true_var) * 100

cat(sprintf(
  "Rolling %d-step Forecast → MAE=%.4f, RMSE=%.4f, MAPE=%.2f%%\n",
  horizon, MAE, RMSE, MAPE
))
# Visualize: black = true variance, red = forecast variance
t_idx <- (train_len+1):n_total
plot(t_idx, true_var, type="l", lwd=2,
     ylab="Variance", xlab="t",
     main="Rolling 1-step Forecast (no X)")
lines(t_idx, pred_var, col="firebrick", lwd=2)
legend("topright", legend=c("True variance","Forecast variance"),
       col=c("black","firebrick"), lwd=2)

```
 
## Project Summary: SynthVolForecast Volatility Modeling Module

### 1. Project Objective

The core goal of this stage is to **develop and validate an automated GARCH/GARCH-X modeling and volatility forecasting pipeline** as a submodule for SynthVolForecast (Synthetic Control Volatility Forecasting). This provides the basis for accurate time-varying variance estimation for synthetic control and causal inference tasks.

---

### 2. Completed Work

#### 2.1 GARCH/GARCH-X Model Development & Automation

- Developed the `auto_garchx` function supporting both classic GARCH and GARCH-X (with exogenous variables), including:
  - Automated order (p, q) selection via information criteria (BIC),
  - Unconditional variance backcasting,
  - Two-step refit for improved stability.

- Simulated various scenarios to validate model performance, including:
  - Pure GARCH and GARCH-X processes,
  - Different parameter regimes and covariate structures,
  - Model stability, parameter recovery, and error behavior.

#### 2.2 Rolling Forecast Pipeline & Error Metrics

- Built a **rolling 1-step-ahead conditional variance forecast** process:
  - Dynamic refit and prediction at each time step,
  - Evaluation using MAE, RMSE, MAPE,
  - Visualization: real (black) vs. predicted (red) variance series.

- Explored model behavior with and without exogenous regressors.

#### 2.3 Code Refactoring & Documentation

- Added detailed English comments throughout core functions (`auto_garchx`, `dbw`, `SynthVolForecast`) for reproducibility and collaboration.
- Organized experiments and outputs for future integration and reporting.

---

### 3. Key Insights

- The workflow robustly fits both GARCH and GARCH-X models, with stable order selection and solid forecasting accuracy.
- `auto_garchx` is flexible, automatic, and ready to embed into the larger SynthVolForecast workflow.
- Rolling forecast pipelines provide an empirical foundation for parameter tuning and window length selection.

---

### 4. Next Steps

1. **Integrate with SynthVolForecast**  
   - Embed the GARCH-based volatility submodule into the overall synthetic control pipeline for both target and donor series.
2. **Expand to High-Dimensional Covariates**
   - Test GARCH-X with higher-dimensional exogenous regressors and more complex settings.
3. **Apply to Real-World Data**
   - Validate on real and more challenging synthetic datasets, and fine-tune methodology.
4. **Documentation and Visualization**
   - Optimize code comments, experiment logging, and result visualization for publication and collaboration.
5. **Academic Preparation**
   - Begin writing methods and results for papers, slides, or thesis documentation.

---

### 5. One-Sentence Summary

This phase established an automated, robust GARCH volatility modeling pipeline to serve as the dynamic backbone for SynthVolForecast, enabling reliable, time-varying variance estimation for advanced synthetic control and causal inference tasks.
 